I"Kﬂ<p>Como continua√ß√£o direta da <a href="/redes-neurais/">parte 12</a> <a href="/categorias/#machine-learning-learning-from-data">dessa s√©rie de textos</a>, vamos, ao longo desse post, implementar uma rede neural utilizando Python. E, para test√°-la, vamos resolver um problema de classifica√ß√£o bin√°ria de dados que <strong>n√£o</strong> s√£o linearmente separ√°veis $-$ aqui, n√£o faremos como na <a href="/transformacoes-nao-lineares/">parte 06</a>, onde utilizamos transforma√ß√µes expl√≠citas sobre o conjunto de dados. Hoje, as transforma√ß√µes ser√£o resultado do processo de aprendizado da rede.</p>

<p>Diferente dos outros textos nos quais implementamos algoritmos de <em>machine learning</em> $-$ e.g., Perceptron, Regress√£o Linear, Regress√£o Log√≠stica, etc., nessa postagem vamos adotar uma ordem diferente para as coisas. Vamos come√ßar com o conjunto de dados com o qual vamos trabalhar.</p>

<p>Nesse caso, ao inv√©s de utilizarmos algum banco de dados conhecido, ou mesmo de simularmos algo que nos daria a caracter√≠stica desejada, vamos utilizar um <em>data set</em> que vem da biblioteca Sklearn. Perceba que <strong>n√£o</strong> iremos utilizar qualquer implementa√ß√£o de algoritmo pela biblioteca; aqui, <strong>apenas</strong> o banco de dados ser√° considerado. Importando as bibliotecas que iremos utilizar e plotando o gr√°fico dos dados que obtivemos, temos o seguinte:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">noise</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">)</span> <span class="c1"># generate non-linear separable data
</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s">"red"</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">"o"</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"Class A"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span>  <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span>  <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s">"green"</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">"s"</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"Class B"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"$x_1$"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"$x_2$"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/implementacao-redes-neurais_files/implementacao-redes-neurais_3_0.png" alt="png" /></p>

<p>Aqui, o banco <code class="highlighter-rouge">make_moons</code> gera dados, como o nome sugere, que tem comportamento gr√°fico parecido com duas meias-luas. O par√¢metro <code class="highlighter-rouge">noise</code> especifica o ru√≠do sobre esse padr√£o. A √∫nica observa√ß√£o √© que, ao inv√©s de usarmos a classifica√ß√£o de <code class="highlighter-rouge">0</code> ou <code class="highlighter-rouge">1</code>, a <code class="highlighter-rouge">Class A</code> ficou rotulada como <code class="highlighter-rouge">-1</code> e a <code class="highlighter-rouge">Class B</code>, como <code class="highlighter-rouge">+1</code>.</p>

<p>Agora vamos √† nossa classe que implementa o algoritmo: <code class="highlighter-rouge">NeuralNetwork</code>. O c√≥digo √© grande, ent√£o, para conseguirmos estud√°-lo adequadamente, vou inser√≠-lo por completo agora, e, na sequ√™ncia, comentamos parte por parte.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">NeuralNetwork</span><span class="p">:</span>
    <span class="s">"""
    Neural Network Model
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">eta</span> <span class="o">=</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="c1"># vector with layers dimensions - including input (0) and output (L)
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># 'L' equals to the number of layers, which does not count input (0)
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">eta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">=</span> <span class="n">epoch</span>
    
    
    <span class="k">def</span> <span class="nf">initialize_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">rn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># it will include 'bias'
</span>        
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">L</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="s">'W'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">rn</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">[</span><span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">[</span><span class="n">l</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">[</span><span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>
    
    
    <span class="k">def</span> <span class="nf">theta_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">s</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">s</span><span class="p">))</span>
    
    
    <span class="k">def</span> <span class="nf">forward_propagation_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_n</span><span class="p">):</span>
        <span class="n">X_n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">X_n</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">s_</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_</span><span class="p">[</span><span class="s">'x0'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_n</span>
        
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">L</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">s_</span><span class="p">[</span><span class="s">'s'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="s">'W'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_</span><span class="p">[</span><span class="s">'x'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">x_</span><span class="p">[</span><span class="s">'x'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s_</span><span class="p">[</span><span class="s">'s'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)])]</span>
            
        <span class="bp">self</span><span class="o">.</span><span class="n">x_</span><span class="p">[</span><span class="s">'x'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_</span><span class="p">[</span><span class="s">'x'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">)][</span><span class="mi">1</span><span class="p">:]</span> <span class="c1"># do not use extra '1' at the last layer
</span>        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_</span><span class="p">[</span><span class="s">'x'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">)]</span>
    
    
    <span class="k">def</span> <span class="nf">back_propagation_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_n</span><span class="p">,</span> <span class="n">y_n</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_</span><span class="p">[</span><span class="s">'d'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_</span><span class="p">[</span><span class="s">'x'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">)]</span> <span class="o">-</span> <span class="n">y_n</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_</span><span class="p">[</span><span class="s">'x'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">)])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span> <span class="c1"># reverse order iteration, from 'L - 1' to 1 (inclusive)
</span>            <span class="n">t_prime</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_</span><span class="p">[</span><span class="s">'x'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)],</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_</span><span class="p">[</span><span class="s">'x'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)])[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">d_</span><span class="p">[</span><span class="s">'d'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">t_prime</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="s">'W'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)],</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_</span><span class="p">[</span><span class="s">'d'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]))[</span><span class="mi">1</span><span class="p">:])</span>          
           
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epoch</span><span class="p">):</span>

            <span class="n">error_in</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">g_</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span> <span class="c1"># initialize 'G' matrix with all entries being zero 
</span>                <span class="bp">self</span><span class="o">.</span><span class="n">g_</span><span class="p">[</span><span class="s">'G'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">item</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="s">'W'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">item</span><span class="p">)]</span>

            <span class="k">for</span> <span class="n">X_n</span><span class="p">,</span> <span class="n">y_n</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">forward_propagation_</span><span class="p">(</span><span class="n">X_n</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">back_propagation_</span><span class="p">(</span><span class="n">X_n</span><span class="p">,</span> <span class="n">y_n</span><span class="p">)</span>
                <span class="n">error_in</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_</span><span class="p">[</span><span class="s">'x'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">)]</span> <span class="o">-</span> <span class="n">y_n</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

                <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">L</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="n">g_X_n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_</span><span class="p">[</span><span class="s">'x'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_</span><span class="p">[</span><span class="s">'x'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_</span><span class="p">[</span><span class="s">'d'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_</span><span class="p">[</span><span class="s">'d'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="c1"># it uses a trick to reshape lists to perform a dot product
</span>                    <span class="bp">self</span><span class="o">.</span><span class="n">g_</span><span class="p">[</span><span class="s">'G'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">g_</span><span class="p">[</span><span class="s">'G'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">g_X_n</span>

            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span> <span class="c1"># update weights
</span>                <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="s">'W'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">item</span><span class="p">)]</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">g_</span><span class="p">[</span><span class="s">'G'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">item</span><span class="p">)]</span>
            
            <span class="k">if</span> <span class="n">n</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="k">print</span><span class="p">(</span><span class="s">"Epoch n. {:4d}: {:.8f}."</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">error_in</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
            
        <span class="k">return</span> <span class="n">error_in</span> <span class="c1"># return final 'error_in'
</span>            
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">X</span><span class="p">:</span>
            <span class="n">prediction</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">forward_propagation_</span><span class="p">(</span><span class="n">item</span><span class="p">))</span>
            
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">prediction</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Vamos come√ßar pelo m√©todo <code class="highlighter-rouge">__init__()</code>. Nesse caso, o par√¢metro <code class="highlighter-rouge">dim</code> √© uma lista com as dimens√µes em cada uma das <em>layers</em> da nossa rede neural $-$ de $0$ a $L$. <code class="highlighter-rouge">L</code> √©, claramente, o n√∫mero de camadas (lembre-se de que a contagem, consiredando a camada de entrada, come√ßa do $0$). <code class="highlighter-rouge">eta</code> (ou $\eta$) √© a taxa de aprendizagem; e, por fim, <code class="highlighter-rouge">epoch</code> √© o n√∫mero de itera√ß√µes que vamos considerar para o nosso processo de otimiza√ß√£o do termo $E_{in}$.</p>

<p>O m√©todo <code class="highlighter-rouge">initialize_()</code> inicializa o vetor de pesos adequadamente $-$ aqui, √© interessante a estrat√©gia que escolhemos tomar para indexar cada uma das <em>layes</em> (a inspira√ß√£o veio <a href="https://www.freecodecamp.org/news/building-a-neural-network-from-scratch/">desse</a> artigo). A fun√ß√£o <code class="highlighter-rouge">theta()</code> √© o nosso n√≥ de transforma√ß√£o n√£o-linear, definido por $\tanh(\cdot)$.</p>

<p>O m√©todo <code class="highlighter-rouge">forward_propagation_()</code>, bem como <code class="highlighter-rouge">back_propagation_()</code> implementam <strong>diretamente</strong> o algoritmo apresentado na <a href="/redes-neurais/">parte 12</a> $-$ a primeira fun√ß√£o calcula o resultado <script type="math/tex">h(\mathbf{x})</script> para cada par <script type="math/tex">(\mathbf{x}_n, y_n)</script>, enquanto que a segunda determina o vetor <script type="math/tex">\mathbf{\delta}^{(l)}</script> (utilizado para, atrav√©s do c√°lculo de <script type="math/tex">\nabla E_{in}</script>, modifica√ß√£o do vetor de pesos).</p>

<p>Agora, perceba que o m√©todo <code class="highlighter-rouge">fit()</code> foi implementada <strong>sem</strong> utilizar a t√©cnica de <em>Stochastic Gradient Descent</em> (SGD). Ou seja, o vetor gradiente $\nabla E_{in}$ considera <strong>todos</strong> os pontos em $\mathcal{D}$ antes de cada atualiza√ß√£o de $\mathbf{w}$. Isso foi feito intencialmente j√° que, logo abaixo, faremos a implementa√ß√£o desse mesmo m√©todo utilizando o SGD para conseguirmos estabelecer a diferen√ßa de tempo computacional requerida por cada uma dessas alternativas.</p>

<p>Por fim, o m√©todo <code class="highlighter-rouge">predict()</code>, como j√° temos feito h√° algum tempo, determina $h(\mathbf{x})$ para cada nova observa√ß√£o. Dito isso, vamos utilizar a classe que acabamos de criar:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>

<span class="n">my_nn</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">eta</span> <span class="o">=</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">)</span>
<span class="n">error_in</span> <span class="o">=</span> <span class="n">my_nn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Epoch n. {:4d}: {:.8f}."</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">my_nn</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">error_in</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch n.    0: 1.01090254.
Epoch n. 1000: 0.34815072.
Epoch n. 2000: 0.34731439.
Epoch n. 3000: 0.34721020.
Epoch n. 4000: 0.34710094.
Epoch n. 5000: 0.34697082.
Epoch n. 6000: 0.34678815.
Epoch n. 7000: 0.34647321.
Epoch n. 8000: 0.34577426.
Epoch n. 9000: 0.34355829.
Epoch n. 10000: 0.32932046.
CPU times: user 5min 29s, sys: 218 ms, total: 5min 30s
Wall time: 5min 30s
</code></pre></div></div>

<p>Antes de qualquer outra coisa, percebe que utilizamos o comando <code class="highlighter-rouge">%%time</code>, que, no Jupyter Notebook, serve para medir o tempo que uma c√©lula demora para ser executada. Nesse caso, note que, para ajustar um modelo de $3$ hidden layers com $16$ n√≥s em cada uma delas, para um total de dez mil itera√ß√µes, gastamos um tempo de 5 minutos e 30 segundos. Lembre-se de que ainda <strong>n√£o</strong> estamos utilizando o <em>Stochastic Gradient Descent</em>. Uma outra observa√ß√£o importante vem do fato de que, por quase nove mil itera√ß√µes, o termo $E_{in}$ praticamente n√£o foi diminu√≠do, nos levando a ter que estender a simula√ß√£o por muito mais tempo (o problema dos m√≠nimos locais, discutido na <a href="/redes-neurais/">parte 12</a> pode ter sido uma das raz√µes desse fen√¥meno).</p>

<p>Ajustado o modelo, podemos, utilizando mais uma vez a fun√ß√£o <code class="highlighter-rouge">plot_decision_regions</code>, determinar as regi√µes de classifica√ß√£o para o nosso problema.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">resolution</span> <span class="o">=</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="n">plot_lim</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">):</span>
    <span class="c1"># general settings
</span>    <span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s">"o"</span><span class="p">,</span> <span class="s">"s"</span><span class="p">,</span> <span class="s">"*"</span><span class="p">,</span> <span class="s">"x"</span><span class="p">,</span> <span class="s">"v"</span><span class="p">]</span>
    <span class="n">colors</span>  <span class="o">=</span> <span class="p">(</span><span class="s">"red"</span><span class="p">,</span> <span class="s">"green"</span><span class="p">,</span> <span class="s">"blue"</span><span class="p">,</span> <span class="s">"gray"</span><span class="p">,</span> <span class="s">"cyan"</span><span class="p">)</span>
    <span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">plot_lim</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">plot_lim</span>
    <span class="n">x2_min</span><span class="p">,</span> <span class="n">x2_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">plot_lim</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">plot_lim</span>
    <span class="c1"># define a grid
</span>    <span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span><span class="p">,</span> <span class="n">resolution</span><span class="p">),</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x2_min</span><span class="p">,</span> <span class="n">x2_max</span><span class="p">,</span> <span class="n">resolution</span><span class="p">))</span>
    <span class="c1"># classify each grid point
</span>    <span class="n">result</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">xx1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">xx2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="c1"># make a plot
</span>    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">colors</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))],</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span> 
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">value</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">value</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> 
                    <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
                    <span class="n">marker</span> <span class="o">=</span> <span class="n">markers</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
                    <span class="n">label</span> <span class="o">=</span> <span class="n">feature_names</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
                    <span class="n">edgecolor</span> <span class="o">=</span> <span class="s">'black'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Classe A'</span><span class="p">,</span> <span class="s">'Classe B'</span><span class="p">]</span>
<span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">my_nn</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">plot_lim</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Fitted Model with Implementation of Neural Network"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"$x_1$"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"$x_2$"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/implementacao-redes-neurais_files/implementacao-redes-neurais_16_0.png" alt="png" /></p>

<p>Do gr√°fico acima, perceba que as transforma√ß√µes n√£o-lineares aprendidas pela rede <strong>n√£o</strong> foram suficientes para captar corretamente o comportamento dos dados. Mais uma vez, isso se deve ao fato de que o algoritmo <strong>n√£o</strong> foi capaz de minimar, a n√≠veis suficientemente bons, o termo $E_{in}$ $-$ o que √©, obviamente, um grande problema.</p>

<p>Para contornar esse obst√°culo, vamos sobrescrever o m√©todo <code class="highlighter-rouge">fit()</code> utilizando, como novidade, a t√©cnica SGD. Para isso, basta utilizar o conceito de ‚Äúheran√ßa‚Äù e criar uma classe <code class="highlighter-rouge">NeuralNetworkSGD</code> que herda os atributos e m√©todos de <code class="highlighter-rouge">NeuralNetwork</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">NeuralNetworkSGD</span><span class="p">(</span><span class="n">NeuralNetwork</span><span class="p">):</span>
    <span class="s">"""
    Neural Network Model using Stochastic Gradient Descent (S.G.D.)
    """</span>        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">initialize_</span><span class="p">()</span>
        
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        
        
            <span class="n">rand_number</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        
            <span class="n">X_n</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">rand_number</span><span class="p">]</span>
            <span class="n">y_n</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">rand_number</span><span class="p">]</span> 

            <span class="bp">self</span><span class="o">.</span><span class="n">forward_propagation_</span><span class="p">(</span><span class="n">X_n</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">back_propagation_</span><span class="p">(</span><span class="n">X_n</span><span class="p">,</span> <span class="n">y_n</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span> <span class="c1"># update weights
</span>                <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="s">'W'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">item</span><span class="p">)]</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_</span><span class="p">[</span><span class="s">'x'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">item</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_</span><span class="p">[</span><span class="s">'x'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">item</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_</span><span class="p">[</span><span class="s">'d'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">item</span><span class="p">)]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_</span><span class="p">[</span><span class="s">'d'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">item</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
              
        <span class="n">error_in</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="k">for</span> <span class="n">X_n</span><span class="p">,</span> <span class="n">y_n</span> <span class="ow">in</span> <span class="nb">zip</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>            
            <span class="n">error_in</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">forward_propagation_</span><span class="p">(</span><span class="n">X_n</span><span class="p">)</span> <span class="o">-</span> <span class="n">y_n</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">error_in</span>
</code></pre></div></div>

<p>Agora, como dito antes, somos capazes de ajustar o vetor de pesos $\mathbf{w}$ para cada rodada de itera√ß√£o que considera <strong>somente</strong> um par $(\mathbf{x}_n, y_n)$, uniformemente escolhido em $\mathcal{D}$. Ajustando o modelo:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">time</span>

<span class="n">my_nn_mod</span> <span class="o">=</span> <span class="n">NeuralNetworkSGD</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">eta</span> <span class="o">=</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="n">epoch</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">)</span>
<span class="n">error_in</span>  <span class="o">=</span> <span class="n">my_nn_mod</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Epoch n. {:4d}: {:.8f}."</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">my_nn_mod</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">error_in</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch n. 10000: 0.01763535.
CPU times: user 50.7 s, sys: 30.5 s, total: 1min 21s
Wall time: 21.8 s
</code></pre></div></div>

<p>Note que, primeiro, mesmo considerando uma rede razoavelmente maior $-$ $3$ hidden layers, s√≥ que dessa vez com $256$ n√≥s em cada uma delas (para as mesmas dez mil itera√ß√µes) $-$ o tempo de execu√ß√£o foi bem menor: 1 minuto e 21 segundos. Al√©m disso o termo $E_{in}$ foi minimizado at√© $\approx 0.0178$. Agora, vamos plotar o gr√°fico das regi√µes de decis√£o para essa segunda simula√ß√£o.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Classe A'</span><span class="p">,</span> <span class="s">'Classe B'</span><span class="p">]</span>
<span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">my_nn_mod</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">plot_lim</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Fitted Model with Implementation of N.N. using S.G.D."</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"$x_1$"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"$x_2$"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/implementacao-redes-neurais_files/implementacao-redes-neurais_23_0.png" alt="png" /></p>

<p>Note que que as regi√µes de decis√£o desenhadas representam muito melhor os dados do que o primeiro caso. O que nos leva a pensar que, de fato, a t√©cnica de <em>Stochastic Gradient Descent</em> gera bons resultados. H√° um problema, entretanto; como conversamos na <a href="/vies-variancia-tradeoff/">parte 10</a>, modelos mais complexos (ou seja, modelos que se adaptam melhor ao conjunto de dados fornecido) podem sofrer grande varia√ß√£o √† medida que variamos $\mathcal{D}$ $-$ nesse caso, podemos sofrer de <em>overfitting</em>, situa√ß√£o na qual o modelo perde seu poder de generaliza√ß√£o para dados fora de $\mathcal{D}$.</p>

<h2 id="conclus√£o">Conclus√£o</h2>

<p>Seguindo √† risca a teoria que desenvolvemos na <a href="/redes-neurais/">parte 12</a>, foi relativamente f√°cil implementarmos um rede neural que aceita par√¢metros bastante razo√°veis sobre sua arquitetura. Vimos que, a depender do conjunto de dados com o qual estamos trabalhando, as transforma√ß√µes n√£o-lineares aprendidas pela rede podem n√£o ser boas o suficiente para descrevermos $\mathcal{D}$ $-$ isso acontece, dentre outros motivos, pelo fato de que minimizar $E_{in}$ pode ser complicado √† medida que encontramos m√≠nimios locais que n√£o s√£o globais. Por√©m, como tamb√©m pudemos testar, esse problema pode ser mitigado empregando a t√©cnica de <em>Stochastic Gradient Descent</em> $-$ que, no nosso caso, gerou bons resultados.</p>

<p>Qualquer d√∫vida, sugest√£o ou <em>feedback</em>, por favor, deixe um coment√°rio abaixo.</p>

<blockquote>
  <p>Essa postagem faz parte de uma <a href="/categorias/#machine-learning-learning-from-data">s√©rie</a> de textos que tem o objetivo de estudar, principalmente, o curso ‚Äú<a href="http://www.work.caltech.edu/telecourse.html">Learning from Data</a>‚Äù, ministrado pelo Professor Yaser Abu-Mostafa. Outros materiais utilizados ser√£o sempre referenciados.</p>
</blockquote>
:ET