I"Ć<p>Na <a href="/memorizar-nao-e-aprender/">parte 03</a> <a href="/categorias/#machine-learning-learning-from-data">dessa série de textos</a>, discutimos se é possível que nossos algoritmos, de fato, aprendam; ou seja, se conseguimos determinar uma função $g \in \mathcal{H}$ que aproxima bem $f$ para pontos $\mathbf{x} \not\in \mathcal{D}$. Esse assunto ainda não foi esgotado, mas por hora vamos nos concentrar em estudar mais alguns algoritmos $-$ em especial, alguns modelos lineares.</p>

<p>Esse post vai ser um pouco diferente, no sentido de mesclar a apresentação teórica com a implementação prática (em Python) de um dos algoritmos de interesse: o <em>Pocket Learning Algorithm</em> (ou só Pocket) $-$ uma variação do Perceptron.</p>

<p>Fazendo referência ao PLA (<em>Perceptron Learning Algorithm</em>), que estudamos nas partes <a href="/o-que-e-aprendizado/">01</a> e <a href="/implementacao-perceptron/">02</a>, uma de suas limitações era a de que os dados precisavam ser linearmente separáveis; caso contrário, o algoritmo não era capaz de classificar corretamente todos os pontos $\mathbf{x} \in \mathcal{D}$, e, por consequência, não convergia. Perceba que essa é uma suposição bem forte, já que, na prática, os dados quase nunca tem essa característica. Uma solução para contornar esse problema seria a de limitar o número de iterações que o algoritmo poderia realizar antes de ser interrompido. Porém, dessa solução, surge um problema.</p>

<p>Lembre-se de que o Perceptron, na tentativa de ajustar o hiperplano definido por <script type="math/tex">\mathbf{w}</script> que classifica corretamente um ponto <script type="math/tex">\mathbf{x}_i</script>, poderia “bagunçar” a classificação associada aos demais pontos. Em outras palavras, mais iterações <strong>não</strong> se traduzem em uma reta (para o caso de <script type="math/tex">2</script> dimensões) “melhor” (no sentido de ter <script type="math/tex">E_{in}(h)</script> menor). Dito isso, uma ideia para tratar esse problema seria a de, a cada etapa do processo, verificar o erro <em>in-sample</em> e, nas situações nas quais ele for o menor, tomar o vetor <script type="math/tex">\mathbf{w}</script> associado como o “escolhido”. Isso é exatamente o que o <em>Pocket Learning Algorithm</em> faz, veja o código a seguir:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Raw implementation
</span><span class="k">class</span> <span class="nc">Pocket</span><span class="p">:</span>
    <span class="s">"""
    Pocket learning algorithm implementation
    """</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">random_seed</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_iterations</span> <span class="o">=</span> <span class="mi">100</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">eta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_seed</span> <span class="o">=</span> <span class="n">random_seed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iterations</span> <span class="o">=</span> <span class="n">n_iterations</span> 
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">rn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_seed</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_</span> <span class="o">=</span> <span class="n">rn</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">errors</span> <span class="o">=</span> <span class="bp">True</span>
        
        <span class="n">partial_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">partial_error_in_sample</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">error_in_sample_</span> <span class="o">=</span> <span class="mi">0</span> 
        
        <span class="k">while</span> <span class="n">errors</span> <span class="ow">and</span> <span class="p">(</span><span class="n">counter</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iterations</span><span class="p">):</span>
            <span class="n">errors</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">error_freq</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">X_i</span><span class="p">,</span> <span class="n">y_i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">y_i</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_i</span><span class="p">):</span>
                    <span class="c1"># update weights for misclassified points
</span>                    <span class="n">update</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">y_i</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+=</span> <span class="n">update</span> <span class="o">*</span> <span class="n">X_i</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">update</span>
                    <span class="n">errors</span> <span class="o">+=</span> <span class="mi">1</span>    
            <span class="k">for</span> <span class="n">X_i</span><span class="p">,</span> <span class="n">y_i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">y_i</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_i</span><span class="p">):</span>
                    <span class="c1"># count misclassified points AFTER analyze all of them 
</span>                    <span class="n">error_freq</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">partial_error_in_sample</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">error_freq</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">counter</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">partial_error_in_sample</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">error_in_sample_</span><span class="p">):</span>
                <span class="c1"># update smallest error and best weights vector
</span>                <span class="bp">self</span><span class="o">.</span><span class="n">error_in_sample_</span> <span class="o">=</span> <span class="n">partial_error_in_sample</span>  
                <span class="n">partial_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
            
        <span class="bp">self</span><span class="o">.</span><span class="n">w_</span> <span class="o">=</span> <span class="n">partial_w</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_i</span><span class="p">):</span>
        <span class="n">eval_func</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">eval_func</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>A classe <code class="highlighter-rouge">Pocket</code> é muito similar à classe <code class="highlighter-rouge">Perceptron</code> que havíamos criado anteriormente; o que faz sentido, já que o Pocket é uma modificação do Perceptron. A principal diferença está no método <code class="highlighter-rouge">fit()</code>. Perceba que agora o número de iterações não é definido apenas pela quantidade de vezes que o vetor $\mathbf{w}$ tem que ser atualizado para que todos os pontos sejam corretamente classificados $-$ na verdade isso pode nem acontecer, já que, como dito, a suposição de que os dados são linearmente separáveis não é mais necessária. O atributo <code class="highlighter-rouge">n_iterations</code> cuida desse limite máximo.</p>

<p>Note também que, ao final de cada ciclo em que o algoritmo percorre todos os pontos $\mathbf{x} \in \mathcal{D}$, o erro $E_{in}(h)$ (erro <em>in-sample</em>) é calculado e armazenado na variável <code class="highlighter-rouge">partial_error_in_sample</code>. Depois disso, no caso de ele ser o menor erro encontrado até o momento, essa quantidade é salva no atributo <code class="highlighter-rouge">error_in_sample</code> $-$ bem como o vetor $\mathbf{w}$, que é armazenado em <code class="highlighter-rouge">partial_w</code>. Ao final, o vetor de pesos escolhido é aquele que teve o menor erro associado; ou seja, o vetor salvo em <code class="highlighter-rouge">partial_w</code>, que é então tranferido para o atributo <code class="highlighter-rouge">w_</code>.</p>

<p>Vamos ver agora como isso funciona em um conjunto de dados que <strong>não</strong> é linearmente separável. Considere o seguinte cenário: suponha que você quer classificar corretamente digitos númericos escritos a mão, como os mostrados na figura abaixo. Cada uma das imagens é composta por uma grade de $16 \times 16$ pixels que assume valores entre $0$ e $255$; ou seja, teríamos um espaço Euclidiano $256$-dimensional de funções (possivelmente) real-avaliadas.</p>

<p><img src="/assets/images/modelos-lineares-de-classificacao-e-pocket_files/digitos-escritos-a-mao.png" alt="Digitos escritos a mão" />
<em>Figura 1 [fonte: “<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>”] $-$ Dígitos numéricos escritos a mão.</em></p>

<p>A fim de diminuir essa quantidade de <em>features</em> (ou características, ou variáveis independentes, etc.), podemos considerar, apenas, alguma medida de intensidade e alguma medida de simetria dos pixels. Obviamente não estamos utilizando todas as informações, mas isso já deve ser o suficiente para termos bons resultados. No código a seguir, ajustaremos um modelo para classificar os dígitos $1$ e $5$ (lembre-se que o Pocket ainda é um classificador binário); veja:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"http://www.amlbook.com/data/zip/features.train"</span><span class="p">,</span> <span class="n">header</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">delimiter</span> <span class="o">=</span> <span class="s">r"\s+"</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'number'</span><span class="p">,</span> <span class="s">'intensity'</span><span class="p">,</span> <span class="s">'symmetry'</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s">'number'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'number'</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span> 
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>number</th>
      <th>intensity</th>
      <th>symmetry</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>0.341092</td>
      <td>-4.528937</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>0.444131</td>
      <td>-5.496812</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>0.231002</td>
      <td>-2.886750</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mask</span> <span class="o">=</span> <span class="p">((</span><span class="n">df</span><span class="o">.</span><span class="n">number</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">number</span> <span class="o">==</span> <span class="mi">5</span><span class="p">))</span> <span class="c1"># select only numbers '1' and '5'
</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>number</th>
      <th>intensity</th>
      <th>symmetry</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5</td>
      <td>0.444131</td>
      <td>-5.496812</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>0.123043</td>
      <td>-0.707875</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>0.113859</td>
      <td>-0.931375</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1561 entries, 0 to 1560
Data columns (total 3 columns):
number       1561 non-null int64
intensity    1561 non-null float64
symmetry     1561 non-null float64
dtypes: float64(2), int64(1)
memory usage: 36.7 KB
</code></pre></div></div>

<p>Perceba que, depois de filtrar adequadamente o conjunto de dados, temos uma base com 1561 entradas que apresentam características (de intensidade e simetria dos pixels) dos números $1$ e $5$. Vamos, agora, plotar esses dados.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s">'intensity'</span><span class="p">,</span> <span class="s">'symmetry'</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s">'number'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># maps 1 to -1, and 5 to 1
</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s">"red"</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">"o"</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"Number 1"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span>  <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span>  <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s">"green"</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">"s"</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"Number 5"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Intensity measure"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Symmetry measure"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/modelos-lineares-de-classificacao-e-pocket_files/modelos-lineares-de-classificacao-e-pocket_14_0.png" alt="png" /></p>

<p>A primeira coisa a se notar é que os dados <strong>não</strong> são linearmente separáveis. Dito isso, vamos, finalmente, ajustar o modelo:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_pocket</span> <span class="o">=</span> <span class="n">Pocket</span><span class="p">(</span><span class="n">n_iterations</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">my_pocket</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Weights vector: {}."</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">my_pocket</span><span class="o">.</span><span class="n">w_</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Smallest error in-sample: {}."</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">my_pocket</span><span class="o">.</span><span class="n">error_in_sample_</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Weights vector: [-9.98375655 -1.494057   -4.21659422].
Smallest error in-sample: 0.0038436899423446506.
</code></pre></div></div>

<p>Nesse caso, estamos considerando o vetor $\mathbf{w}$ associado ao menor erro $E_{in}(h)$ encontrado: <code class="highlighter-rouge">0.0038436899</code>. Podemos, então, plotar o gráfico com as regiões de decisão; utilizando, para isso, a função <code class="highlighter-rouge">plot_decision_regions</code> já discutida na <a href="/implementacao-perceptron/">parte 02</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">resolution</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">plot_lim</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">):</span>
    <span class="c1"># general settings
</span>    <span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s">"o"</span><span class="p">,</span> <span class="s">"s"</span><span class="p">,</span> <span class="s">"*"</span><span class="p">,</span> <span class="s">"x"</span><span class="p">,</span> <span class="s">"v"</span><span class="p">]</span>
    <span class="n">colors</span>  <span class="o">=</span> <span class="p">(</span><span class="s">"red"</span><span class="p">,</span> <span class="s">"green"</span><span class="p">,</span> <span class="s">"blue"</span><span class="p">,</span> <span class="s">"gray"</span><span class="p">,</span> <span class="s">"cyan"</span><span class="p">)</span>
    <span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">plot_lim</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">plot_lim</span>
    <span class="n">x2_min</span><span class="p">,</span> <span class="n">x2_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">plot_lim</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">plot_lim</span>
    <span class="c1"># define a grid
</span>    <span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span><span class="p">,</span> <span class="n">resolution</span><span class="p">),</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x2_min</span><span class="p">,</span> <span class="n">x2_max</span><span class="p">,</span> <span class="n">resolution</span><span class="p">))</span>
    <span class="c1"># classify each grid point
</span>    <span class="n">result</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">xx1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">xx2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="c1"># make a plot
</span>    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">colors</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))],</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span> 
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">value</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">value</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> 
                    <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
                    <span class="n">marker</span> <span class="o">=</span> <span class="n">markers</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
                    <span class="n">label</span> <span class="o">=</span> <span class="n">feature_names</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
                    <span class="n">edgecolor</span> <span class="o">=</span> <span class="s">'black'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Number 1'</span><span class="p">,</span> <span class="s">'Number 5'</span><span class="p">]</span>
<span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">my_pocket</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">plot_lim</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Fitted model with Pocket Learning Algorithm"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Intensity measure"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Symmetry measure"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/modelos-lineares-de-classificacao-e-pocket_files/modelos-lineares-de-classificacao-e-pocket_19_0.png" alt="png" /></p>

<p>Como podemos observar, o Pocket encontrou uma reta que minimiza (considerando as cem primeiras iterações) o erro amostral $-$ o que, como discutido na <a href="/memorizar-nao-e-aprender/">parte 03</a>, é um dos passos necessários para dizermos que o nosso algoritmo aprendeu.</p>

<h2 id="conclusão">Conclusão</h2>

<p>Vimos ao longo do texto um generalização para o Perceptron, chamada Pocket. Esses dois algoritmos fazem parte de uma classe maior de modelos lineares, e são, portanto, classificadores lineares. No próximo post vamos estudar o modelo de regressão $-$ nesse caso, a função alvo $f: \mathcal{X} \rightarrow \mathcal{Y}$ terá contradominio nos números reais.</p>

<p>Qualquer dúvida, sugestão ou <em>feedback</em>, por favor, deixe um comentário abaixo.</p>

<blockquote>
  <p>Essa postagem faz parte de uma <a href="/categorias/#machine-learning-learning-from-data">série</a> de textos que tem o objetivo de estudar, principalmente, o curso “<a href="http://www.work.caltech.edu/telecourse.html">Learning from Data</a>”, ministrado pelo Professor Yaser Abu-Mostafa. Outros materiais utilizados serão sempre referenciados.</p>
</blockquote>
:ET