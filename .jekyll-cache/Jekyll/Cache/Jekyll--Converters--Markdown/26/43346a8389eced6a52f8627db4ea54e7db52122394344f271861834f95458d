I"fs<p>Continuando a discussão sobre modelos lineares, assunto que começamos a estudar na <a href="/modelos-lineares-de-classificacao-e-pocket/">parte 04</a> <a href="/categorias/#machine-learning-learning-from-data">dessa série de textos</a>, vamos, nesse post, falar do modelo de regressão. A principal diferença entre esse tipo de modelo e os classificadores que estudamos anteriormente é que agora a função alvo $f$ é real-avaliada $-$ que é o mesmo que dizer que $\mathcal{Y} \subset \mathbb{R}$.</p>

<p>Para dar contexto ao problema, considere o seguinte exemplo: você é professor de uma turma de Ensino Médio e, ao longo do ano, aplica três provas de 20 pontos cada. Depois de os alunos realizarem os dois primeiros testes, você deseja construir um modelo que lhe permita predizer a nota dos alunos na terceira prova. Perceba que, nesse caso, não estamos tentando “classificar” a terceira nota $-$ aqui, $y \in [0, 20]$. Para resolver esse tipo de tarefa, faremos uso de um modelo (linear) de regressão. <strong>Observação 1:</strong> inúmeras outras características dos alunos poderiam ser utilizadas como preditores (exemplo: horas de estudo, número de faltas ao longo do ano, etc.), mas para simplicidade do modelo, vamos nos atentar, somente, às notas nas duas primeiras avaliações.</p>

<p>Nesse caso, a classe de funções $h \in \mathcal{H}$ será defina por:</p>

<script type="math/tex; mode=display">\begin{align*}
h(\mathbf{x}) = \sum_{i = 0}^{d} w_i \, x_i = \mathbf{w}^{\text{T}}\mathbf{x}.
\end{align*}</script>

<p>Perceba que $h$, como acabamos de definir, é muito parececido com a classe funções que utilizamos quando dicutimos o Perceptron; porém, ao invés do sinal $-$ $\text{sign}(\cdot)$ $-$, estamos interessado no valor de $\mathbf{w}^{\text{T}}\mathbf{x}$.</p>

<p>Continuando, da mesma forma que fizemos quando estudamos o modelo Pocket, o que queremos nesse caso é minimizar o erro amostral $-$ $E_{in}(h)$ $-$, associado ao modelo. Nesse sentido, utilizaremos uma medida de erro clássica para análise de regressão: o <strong>erro quadrático</strong>. Defina $E_{out}(h) = \mathbb{E}\left[(h(\mathbf{x}) - y)^2\right]$ $-$ nesse caso, o valor esperado é calculado com respeito à distruição $P(\mathbf{x}, y)$. Porém, como não temos acesso à medida de erro $E_{out}(h)$, como discutimos na <a href="/memorizar-nao-e-aprender/">parte 03</a>, uma alternativa é minimar o erro <em>in-sample</em>.</p>

<p>Assim, podemos escrever que <script type="math/tex">E_{in}(h) = \frac{1}{N} \sum_{n = 1}^{N}(h(\mathbf{x}_n) - y_n)^2</script>. Nesse sentido, nossa missão é encontrar <script type="math/tex">\mathbf{w}</script> tal que <script type="math/tex">E_{in}(h)</script> é mínimo. Veja:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
E_{in}(\mathbf{w}) & = \frac{1}{N} \sum_{n = 1}^{N} (\mathbf{w}^{\text{T}}\mathbf{x} - y_n)^2 \\
          & = \frac{1}{N} \mid\mid X \mathbf{w} - \mathbf{y} \mid\mid^2 \text{, onde } \mid\mid \cdot \mid\mid \text{ é a norma Euclidiana} \\
          & = \frac{1}{N} \left(\mathbf{w}^{\text{T}} X^{\text{T}} X \mathbf{w} - 2 \mathbf{w}^{\text{T}} X^{\text{T}}\mathbf{y} + \mathbf{y}^{\text{T}} \mathbf{y} \right).
\end{align*} %]]></script>

<p>Derivando $E_{in}(\mathbf{w})$; ou seja, calculando o vetor gradiente $\nabla E_{in}(\mathbf{w})$, obtemos:</p>

<script type="math/tex; mode=display">\begin{align*}
\nabla E_{in}(\mathbf{w}) = \frac{2}{N}\left(X^{\text{T}} X \mathbf{w} - X^{\text{T}} y \right).
\end{align*}</script>

<p>Igualando $\nabla E_{in}(\mathbf{w})$ ao vetor $\mathbf{0}$, temos que:</p>

<script type="math/tex; mode=display">\begin{align*}
X^{\text{T}} X \mathbf{w} = X^{\text{T}} \mathbf{y}.
\end{align*}</script>

<p>Assim, se $X^{\text{T}}X$ for invertível, então $\mathbf{w} = (X^{\text{T}} X)^{-1} X^{\text{T}} \mathbf{y}$; onde $X^{\dagger} = (X^{\text{T}} X)^{-1} X^{\text{T}}$ é conhecida como <em>pseudo-inversa</em> de $X$. Aqui, $(X^{\text{T}} X)^{-1}$ existirá sempre que nenhuma coluna de $X$ for combinação linear das outras; na prática, se $N$ for muito maior que $d + 1$, então isso quase sempre será satisfeito.</p>

<p>Aqui, dois pontos são importantes. <strong>Obervação 2:</strong> note que obtemos uma solução analítica para minimizar $E_{in}(\mathbf{w})$ $-$ para o caso do Pocket, por exemplo, esse não era o caso. <strong>Observação 3:</strong> perceba que $E_{in}(\mathbf{w})$ é função de $\mathbf{w}$, e não de $X$ (que, nessa situação, é constante).</p>

<p>Agora que temos um solução para o problema de minimazação do erro, podemos implementar o algoritmo. A classe <code class="highlighter-rouge">LinearRegression</code> cuida disso.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Raw implementation
</span><span class="k">class</span> <span class="nc">LinearRegression</span><span class="p">:</span>
    <span class="s">"""
    Linear Regression Algorithm
    """</span>
    <span class="k">def</span> <span class="nf">__ini__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])])</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">Z</span><span class="p">,</span> <span class="n">X</span><span class="p">]</span> <span class="c1"># create an array by including each component as a column
</span>        <span class="n">X_dagger</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># find the pseudo-inverse matrix
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">w_</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_dagger</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
        <span class="k">return</span> <span class="bp">self</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<p>Perceba que no método <code class="highlighter-rouge">fit()</code> calculamos a <em>pseudo-inversa</em> de $X$ através da função <code class="highlighter-rouge">pinv()</code> (implementada pelo o módulo de álgebra linear do Numpy). Depois disso, bastou calcularmos o vetor de pesos <code class="highlighter-rouge">w_</code> definido por $\mathbf{w} =  X^{\dagger} \mathbf{y}$. O método <code class="highlighter-rouge">predict()</code> apenas implementa a função $h(\mathbf{x})$.</p>

<p>Agora, para ajustarmos o modelo, considere o exemplo das notas de três provas feitas por alunos do Ensino Médio discutido no começo do texto. Veja os dados:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"student-mat.csv"</span><span class="p">,</span> <span class="n">sep</span> <span class="o">=</span> <span class="s">";"</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s">'G1'</span><span class="p">,</span> <span class="s">'G2'</span><span class="p">,</span> <span class="s">'G3'</span><span class="p">]]</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>G1</th>
      <th>G2</th>
      <th>G3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5</td>
      <td>6</td>
      <td>6</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>5</td>
      <td>6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>7</td>
      <td>8</td>
      <td>10</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 395 entries, 0 to 394
Data columns (total 3 columns):
G1    395 non-null int64
G2    395 non-null int64
G3    395 non-null int64
dtypes: int64(3)
memory usage: 9.4 KB
</code></pre></div></div>

<p>Perceba que temos $395$ entradas não nulas para cada uma das três provas. Nesse caso, <code class="highlighter-rouge">G1</code> e <code class="highlighter-rouge">G2</code> serão nossos preditores, e <code class="highlighter-rouge">G3</code> será fará o papel da variável dependente. Vamos, então, visualizar como os dados se comportam:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s">'G1'</span><span class="p">,</span> <span class="s">'G2'</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s">'G3'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span>  <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span> <span class="o">=</span> <span class="s">'3d'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">"red"</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">"o"</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim3d</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim3d</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlim3d</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

<span class="n">ticks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">ticks</span> <span class="o">=</span> <span class="n">ticks</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">ticks</span> <span class="o">=</span> <span class="n">ticks</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">zaxis</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">ticks</span> <span class="o">=</span> <span class="n">ticks</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"G1"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"G2"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s">"G3"</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">150</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/modelo-de-regressao-linear_files/modelo-de-regressao-linear_17_0.png" alt="png" /></p>

<p>Note que, ao menos a primeira vista, os dados da amostra aparentam ter comportamento linear; o que nos permite prosseguir com o ajuste do modelo:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_regression</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">my_regression</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Weights vector: {}."</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">my_regression</span><span class="o">.</span><span class="n">w_</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Weights vector: [-1.83001214  0.15326859  0.98686684].
</code></pre></div></div>

<p>Veja que o vetor de pesos, nesse caso com $3$ coordenadas ($w_0$, $w_1$ e $w_2$, respectivamente) foi facilmente determinado. Para conseguirmos visualizar o plano definido por esse vetor, podemos plotar o seguinte gráfico:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">g2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">g1</span><span class="p">,</span> <span class="n">g2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">)</span>
<span class="n">g3</span> <span class="o">=</span> <span class="n">my_regression</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">g1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">g2</span><span class="p">)])</span>
<span class="n">g3</span> <span class="o">=</span> <span class="n">g3</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">g1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span>  <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span> <span class="o">=</span> <span class="s">'3d'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">,</span> <span class="n">g3</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">"red"</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">"red"</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">"o"</span><span class="p">,</span> <span class="n">edgecolor</span> <span class="o">=</span> <span class="s">"black"</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim3d</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim3d</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlim3d</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

<span class="n">ticks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">ticks</span> <span class="o">=</span> <span class="n">ticks</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">ticks</span> <span class="o">=</span> <span class="n">ticks</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">zaxis</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">ticks</span> <span class="o">=</span> <span class="n">ticks</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Fitted model with raw implementation"</span><span class="p">,</span> <span class="n">pad</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"G1"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"G2"</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s">"G3"</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">150</span><span class="p">)</span>
<span class="c1"># ax.invert_xaxis()
</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/modelo-de-regressao-linear_files/modelo-de-regressao-linear_21_0.png" alt="png" /></p>

<p>Nesse caso, o plano parece representar bem o conjunto de pontos. Entretanto, ainda não temos uma análise quantitativa desse tipo de medida $-$ como dito em posts anteriores, ainda vamos chegar lá: na discussão da acurácia dos modelos que escrevemos.</p>

<p>Um pequeno teste que podemos fazer é o de predizer, de acordo com o modelo ajustado, qual nota dois alunos arbitrários tirariam na prova <code class="highlighter-rouge">G3</code> (com base em suas notas <code class="highlighter-rouge">G1</code> e <code class="highlighter-rouge">G2</code>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"O aluno A teve notas G1 = 6.5 e G2 = 17. Assim, espera-se que ele tire {:.2f} pontos na última prova."</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">my_regression</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">6.5</span><span class="p">,</span> <span class="mi">17</span><span class="p">]))))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"O aluno B teve notas G1 = 17 e G2 = 6.5. Assim, espera-se que ele tire {:.2f} pontos na última prova."</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">my_regression</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">17</span><span class="p">,</span> <span class="mf">6.5</span><span class="p">]))))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>O aluno A teve notas G1 = 6.5 e G2 = 17. Assim, espera-se que ele tire 15.94 pontos na última prova.


O aluno B teve notas G1 = 17 e G2 = 6.5. Assim, espera-se que ele tire 7.19 pontos na última prova.
</code></pre></div></div>

<p>Uma observação interessante nesse caso é a de que a nota <code class="highlighter-rouge">G3</code> é mais afetada pela nota <code class="highlighter-rouge">G2</code> do que por <code class="highlighter-rouge">G1</code> $-$ o que faz total sentido, já que, como vimos, $w_2 &gt; w_1$.</p>

<p>Por fim, vamos utilizar a implementação do Sklearn para o modelo de regressão linear. Confira o código a seguir:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sklearn usage
</span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">sklearn_regression</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">sklearn_regression</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Weights vector - intercept: {} &amp; coefficients: {}."</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">sklearn_regression</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">sklearn_regression</span><span class="o">.</span><span class="n">coef_</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Weights vector - intercept: -1.8300121405807381 &amp; coefficients: [0.15326859 0.98686684].
</code></pre></div></div>

<p>Nesse caso, como obtivemos uma solução analítica para $\mathbf{w}$, os resultados são exatamente iguais aos que encontramos.</p>

<h2 id="conclusão">Conclusão</h2>

<p>Nesse texto discutimos o importante modelo de regressão linear, que nos permite então, trabalhar com uma função alvo $f: \mathcal{X} \rightarrow \mathcal{Y}$, com $\mathcal{Y} \subset \mathbb{R}$. Como vimos, nesse caso há uma solução analítica para $\mathbf{w}$, o que nem sempre é o caso. Além disso, fizemos a implementação do algoritmo em Python $-$ em contraponto à utilização direta do Sklearn (que também foi feita no final do texto). Na próxima postagem, discutiremos como trabalhar com transformações não lineares.</p>

<p>Qualquer dúvida, sugestão ou <em>feedback</em>, por favor, deixe um comentário abaixo.</p>

<blockquote>
  <p>Essa postagem faz parte de uma <a href="/categorias/#machine-learning-learning-from-data">série</a> de textos que tem o objetivo de estudar, principalmente, o curso “<a href="http://www.work.caltech.edu/telecourse.html">Learning from Data</a>”, ministrado pelo Professor Yaser Abu-Mostafa. Outros materiais utilizados serão sempre referenciados.</p>
</blockquote>
:ET