I"Ô&<p>Continuando com a discuss√£o, introduzida na <a href="/dimensao-vc/">parte 09</a> <a href="/categorias/#machine-learning-learning-from-data">dessa s√©rie de textos</a>, de que deve existir um ‚Äúmeio termo‚Äù sobre o tamanho de $\mathcal{H}$ $-$ lembre-se: se $\mathcal{H}$ √© muito grande, conseguimos diminuir o termo $E_{in}$; por√©m, somos penalizados na generaliza√ß√£o do modelo para dados fora de $\mathcal{D}$. Em contrapartida, se $\mathcal{H}$ √© pequeno, conseguimos, com probabilidade suficientemente alta, que $E_{out} \approx E_{in}$, mas perdemos a capacidade de fazer $E_{in}$ arbitrariamente pequeno $-$, vamos, ao longo dessa postagem, abordar esse problema de um ponto de vista diferente.</p>

<p>Ao inv√©s de cotar (por cima) o termo $E_{out}$ por $E_{in} + \Omega$ (recorde que $\Omega$ pode ser interpretado como uma ‚Äúpenalidade‚Äù aplicada √† cota de $E_{out}$ como resultado da complexidade do modelo escolhido), vamos decompor $E_{out}$ em dois termos diferentes: <strong>vi√©s</strong> e <strong>vari√¢ncia</strong>. Para isso, iremos, como no modelo de regress√£o, trabalhar com uma medida de erro espec√≠fica, o <strong>erro quadr√°tico</strong>. Da <a href="/modelo-de-regressao-linear/">parte 05</a>, temos que <script type="math/tex">E_{out}(h) = \mathbb{E}\left[(h(\mathbf{x}) - f(\mathbf{x}_n))^2\right]</script> e <script type="math/tex">E_{in}(h) = \frac{1}{N} \sum_{n = 1}^{N}(h(\mathbf{x}_n) - f(\mathbf{x}_n))^2</script>, para <script type="math/tex">h \in \mathcal{H}</script>.</p>

<p>Assim, podemos escrever que:</p>

<script type="math/tex; mode=display">\begin{align*}
E_{out}(g^{(\mathcal{D})}) = \mathbb{E}_{\mathbf{x}}\left[(g^{(\mathcal{D})}(\mathbf{x})-f(\mathbf{x}))^2\right],
\end{align*}</script>

<p>onde $g \in \mathcal{H}$ √© a hip√≥tese escolhida por $\mathcal{A}$ e depende, obviamente, do conjunto de dados escolhido $\mathcal{D}$. Al√©m disso, $\mathbb{E}_{\mathbf{x}}\left[\cdot\right]$ √© o valor esperado com respeito a $\mathbf{x}$. Agora, para tirar a depend√™ncia que $g$ tem de $\mathcal{D}$, podemos tomar a esperan√ßa (dessa vez, com respeito a $\mathcal{D}$) dos dois lados da equa√ß√£o $-$ assim, temos que:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\mathbb{E}_{\mathcal{D}}\left[E_{out}(g^{(\mathcal{D})})\right] & = \mathbb{E}_{\mathcal{D}}\left[\mathbb{E}_{\mathbf{x}}\left[(g^{(\mathcal{D})}(\mathbf{x})-f(\mathbf{x}))^2\right]\right] \\
& = \mathbb{E}_{\mathbf{x}}\left[\mathbb{E}_{\mathcal{D}}\left[(g^{(\mathcal{D})}(\mathbf{x})-f(\mathbf{x}))^2\right]\right] \text{, j√° que } (\cdot)^2 > 0 \\
& = \mathbb{E}_{\mathbf{x}}\left[ \mathbb{E}_{\mathcal{D}}\left[g^{(\mathcal{D})}(\mathbf{x})^2\right] - 2 \bar{g}(\mathbf{x}) f(\mathbf{x}) + f(\mathbf{x})^2 \right] \text{, onde } \bar{g}(\mathbf{x}) = \mathbb{E}_{\mathcal{D}}\left[g^{(\mathcal{D})}(\mathbf{x})\right] \\
& = \mathbb{E}_{\mathbf{x}}\left[ \mathbb{E}_{\mathcal{D}}\left[g^{(\mathcal{D})}(\mathbf{x})^2\right] - \bar{g}(\mathbf{x})^2 + \bar{g}(\mathbf{x})^2 - 2 \bar{g}(\mathbf{x}) f(\mathbf{x}) + f(\mathbf{x})^2 \right] \\
& = \mathbb{E}_{\mathbf{x}}\left[ \mathbb{E}_{\mathcal{D}}\left[(g^{(\mathcal{D})}(\mathbf{x}) - \bar{g}(\mathbf{x}))^2\right] + (\bar{g}(\mathbf{x}) - f(\mathbf{x}))^2 \right].
\end{align*} %]]></script>

<p>Nesse caso, vamos dizer que $\text{Vi√©s}(\mathbf{x}) = (\bar{g}(\mathbf{x}) - f(\mathbf{x}))^2$ e que $\text{Var}(\mathbf{x}) = \mathbb{E}_{\mathcal{D}}\left[( g^{(\mathcal{D})}(\mathbf{x}) - \bar{g}(\mathbf{x}))^2\right]$. <strong>Observa√ß√£o 1:</strong> Aqui, a nota√ß√£o e a forma de definir as quantidades de interesse talvez lhe pare√ßa estranho $-$ para mim, esse tamb√©m foi o caso $-$, mas as utilizarei com a inten√ß√£o de seguir o livro <a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>. Dessa forma, temos que:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\mathbb{E}_{\mathcal{D}}\left[E_{out}(g^{(\mathcal{D})})\right] & = \mathbb{E}_{\mathcal{x}}\left[\text{Vi√©s}(\mathbf{x}) + \text{Var}(\mathbf{x}) \right] \\
& = \text{Vi√©s} + \text{Var},
\end{align*} %]]></script>

<p>onde <script type="math/tex">\text{Vi√©s} = \mathbb{E}_{\mathbf{x}}\left[\text{Vi√©s}(\mathbf{x})\right]</script> e <script type="math/tex">\text{Var} = \mathbb{E}_{\mathbf{x}}\left[\text{Var}(\mathbf{x})\right]</script>. Nesse caso, perceba que a decomposi√ß√£o que fizemos assume que <strong>n√£o</strong> existe ru√≠do $-$ caso f√¥ssemos inclu√≠-lo; i.e., se opt√°ssemos por escrever <script type="math/tex">y = f(\mathbf{x}) + \epsilon</script>, ter√≠amos um termo a mais na √∫ltima soma.</p>

<p>Vamos ver agora que o problema associado ao tamanho (ou complexidade) de $\mathcal{H}$ √© capturado pela decomposi√ß√£o de vi√©s-vari√¢ncia de $E_{out}$ que fizemos. Para isso, considere dois cen√°rios extremos:</p>

<ol>
  <li>
    <p>$\mathcal{H} = \lbrace h \rbrace$, tal que $h \neq f$. Nesse caso, como o conjunto de hip√≥teses tem cardinalidade $1$, teremos $g^{(\mathcal{D})}(\mathbf{x}) = \bar{g}(\mathbf{x})$; logo, $\text{Var} = 0$. Por√©m, o termo $\text{Vi√©s}$ depender√° apenas do qu√£o bem $h$ aproxima $f$ $-$ assim, via de regra, teremos vi√©s alto.</p>
  </li>
  <li>
    <p>$\mathcal{H} = \lbrace h_1, \cdots h_k \rbrace$, para $k$ ‚Äúmuito grande‚Äù. Aqui, se $f \in \mathcal{H}$, teremos vi√©s muito pr√≥ximo de zero. Entretanto, apesar de $\bar{g}(\mathcal{x}) \approx f$, a escolha da hip√≥tese $g$ por $\mathcal{A}$ ir√° variar de acordo com $\mathcal{D}$, fazendo com que o termo $\text{Var}$ assuma valores (potencialmente) ‚Äúaltos‚Äù.</p>
  </li>
</ol>

<p><strong>Exemplo:</strong> Suponha que $f(x) = \sin(\pi x)$, e que $\mathcal{D}$ √© tal que $N = 2$; com pontos $(x_1, y_1)$ e $(x_2, y_2)$ tal que $x$ √© amostrado uniformemente de $[-1, 1]$. Para esse exemplo, considere dois conjuntos de hip√≥teses:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\mathcal{H}_0 &: h(x) = b \\
\mathcal{H}_1 &: h(x) = ax + b.
\end{align*} %]]></script>

<p>Para $\mathcal{H}_0$, $b$ √© escolhido de tal forma que $b = \frac{y_1 + y_2}{2}$. Similarmente, para $\mathcal{H}_1$, n√≥s escolhemos $a$ e $b$ de tal forma que a reta definida por esses valores passa por $(x_1, y_1)$ e $(x_2, y_2)$. Assim, atrav√©s de simula√ß√£o, podemos estimar o vi√©s e vari√¢ncia para os modelos que acabamos de definir. Mas antes, veja a figura abaixo $-$ que mostra como as curvas $h(x)$ foram ajustadas para $\mathcal{H}_0$ e $\mathcal{H}_1$ nas v√°rias itera√ß√µes do processo de simula√ß√£o que sorteia os pontos de $\mathcal{D}$:</p>

<p><img src="/assets/images/vies-variancia-tradeoff_files/figura-1.png" alt="Simula√ß√£o-dois-modelos" />
<em>Figura 1 [fonte: ‚Äú<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>‚Äù] $-$ Curva $f(x) = \sin(\pi x)$ para os modelos $\mathcal{H}_0$ e $\mathcal{H}_1$.</em></p>

<p>Perceba, a partir da Fig. 1, que as retas ajustadas para $\mathcal{H}_1$ variam muito mais se comparadas √†s retas de $\mathcal{H}_0$. A imagem a seguir formaliza essa diferen√ßa e sumariza as informa√ß√µes de vi√©s e vari√¢ncia para os dois conjuntos de hip√≥teses:</p>

<p><img src="/assets/images/vies-variancia-tradeoff_files/figura-2.png" alt="Sum√°rio-de-vi√©s-vari√¢ncia" />
<em>Figura 2 [fonte: ‚Äú<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>‚Äù] $-$ An√°lise de vi√©s e vari√¢ncia para $\mathcal{H}_0$ e $\mathcal{H}_1$.</em></p>

<p>Como √© poss√≠vel perceber a partir da Fig. 2, apesar de o modelo mais simples <script type="math/tex">\mathcal{H}_0</script> ter maior vi√©s (se comparado a <script type="math/tex">\mathcal{H}_1</script>), ele tem valor esperado para <script type="math/tex">E_{out}</script> menor (soma de vi√©s e vari√¢ncia) e, portanto, √© o conjunto de hip√≥teses prefer√≠vel nesse cen√°rio. <strong>Observa√ß√£o 2:</strong> como o termo <script type="math/tex">\text{Var}</script> diminui com o aumento de <script type="math/tex">N</script>, caso tiv√©ssemos um amostra maior, o modelo <script type="math/tex">\mathcal{H}_1</script> poderia come√ßar a se tornar a melhor op√ß√£o.</p>

<p>Aqui, o objetivo pr√°tico √© diminuir a vari√¢ncia sem aumentar consideravelmente o vi√©s; ou, de forma an√°loga, diminuir o vi√©s sem aumentar muito a vari√¢ncia. Para alcan√ßar resultados nesse sentido, diversas t√©cnicas podem ser empregadas $-$ <em>regularization</em> √© uma delas.</p>

<h2 id="conclus√£o">Conclus√£o</h2>

<p>A partir da ideia de que a complexidade de um modelo, associada ao tamanho do conjunto de hip√≥teses $\mathcal{H}$, √© um fator complicado de se tratar $-$ $\mathcal{H}$ grande facilita a minimiza√ß√£o de $E_{in}$, mas compromete a generaliza√ß√£o de $f$ por $g$; ao passo que $\mathcal{H}$ pequeno compromete o erro amostral, mas permite $E_{out} \approx E_{in}$ $-$, conseguimos, atrav√©s das quantidades <strong>vi√©s</strong> e <strong>vari√¢ncia</strong>, introduzir um novo tipo de an√°lise. Modelos mais complexos s√£o mais permiss√≠veis em rela√ß√£o ao ajuste da curva aos dados, mas sofrem mais varia√ß√£o com as poss√≠veis amostras para $\mathcal{D}$; por outro lado, modelos mais simples podem ter dificuldade de captar o comportamento de $f$, mas s√£o menos suscet√≠veis √†s varia√ß√µes do conjunto de dados. Esse tipo de an√°lise ser√° feita mais vezes √† medida que formos implementando outros exemplos. Por fim, o pr√≥ximo post voltar√° a falar de modelos lineares.</p>

<p>Qualquer d√∫vida, sugest√£o ou <em>feedback</em>, por favor, deixe um coment√°rio abaixo.</p>

<blockquote>
  <p>Essa postagem faz parte de uma <a href="/categorias/#machine-learning-learning-from-data">s√©rie</a> de textos que tem o objetivo de estudar, principalmente, o curso ‚Äú<a href="http://www.work.caltech.edu/telecourse.html">Learning from Data</a>‚Äù, ministrado pelo Professor Yaser Abu-Mostafa. Outros materiais utilizados ser√£o sempre referenciados.</p>
</blockquote>
:ET