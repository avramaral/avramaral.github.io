I"œ1<p>Continuando a discuss√£o que come√ßamos na <a href="/teoria-da-generalizacao/">√∫ltima parte</a> da <a href="/categorias/#machine-learning-learning-from-data">nossa s√©rie de textos</a>, vamos estudar mais propriedades associadas ao que demos o nome de ‚ÄúTeoria da Generaliza√ß√£o‚Äù; ou, em outras palavras, ao estudo de como os nossos modelos podem ser generalizados para conjuntos de dados fora de $\mathcal{D}$.</p>

<p>Nesse sentido, a primeira defini√ß√£o que vamos introduzir $-$ e a mais importante do ponto de vista <strong>pr√°tico</strong>, √© a de Dimens√£o Vapnik-Chervonenkis (VC).</p>

<p><strong>Defini√ß√£o 1:</strong> a <strong>dimens√£o VC</strong> de um conjunto $\mathcal{H}$, denotada por $d_{\text{VC}}(\mathcal{H})$ (ou somente $d_{\text{VC}}$), √© o maior valor de $N$ para o qual $m_{\mathcal{H}}(N) = 2^N$.</p>

<p>De forma simples, a dimens√£o VC corresponde ao m√°ximo de pontos $N$ (para <strong>algum</strong> conjunto de pontos) que $\mathcal{H}$ pode <em>quebrar</em> (lembre-se do significado que demos √† essa express√£o). Al√©m disso, como √© f√°cil perceber, se $d_{\text{VC}}$ √© dimens√£o VC para $\mathcal{H}$, ent√£o $k = d_{\text{VC}} + 1$ √© <em>break point</em>. Assim, o <strong>Teor. 1 da <a href="/teoria-da-generalizacao/">parte 08</a></strong> pode ser reescrito da seguinte forma:</p>

<script type="math/tex; mode=display">\begin{align*}
m_{\mathcal{H}(N)} \leq \sum_{i = 0}^{d_{\text{VC}}}{N \choose i},
\end{align*}</script>

<p>onde o termo do lado direito da inequa√ß√£o √© um polin√¥mio de grau $d_{\text{VC}}$; i.e., a dimens√£o VC tamb√©m determina o grau do polin√¥mio que limita a fun√ß√£o de crescimento.</p>

<p>Uma outra consequ√™ncia direta dessa nova defini√ß√£o √© que conseguimos, sem surpresa, caracterizar o fen√¥meno $f \approx g$; ou seja, se $d_{\text{VC}}$ √© finito, ent√£o $g \in \mathcal{H}$ ir√° generalizar o comportamento de $f$. Em adi√ß√£o, perceba que a dimens√£o VC √© independente de:</p>

<ul>
  <li>$\mathcal{A}$, o algoritmo de aprendizagem $-$ note que, sob a hip√≥tese de que $d_{\text{VC}}$ √© finito, $f$ √© generalizada (bem ou mal) por qualquer $h \in \mathcal{H}$.</li>
  <li>$P$, a distribui√ß√£o de entrada $-$ nesse caso, perceba que, para a fun√ß√£o de crescimento, escolhemos um conjunto de pontos que maximiza o n√∫meros de dicotomias geradas por $\mathcal{H}$; assim, a escolha da distribui√ß√£o que determina os pontos de $\mathcal{D}$ n√£o exerce papel fundamental na ideia de ‚Äúgeneraliza√ß√£o‚Äù.</li>
  <li>$f$, a fun√ß√£o alvo $-$ veja que, satisfeita determinadas condi√ß√µes, a Desigualdade de Vapnik-Chervonenkis √© v√°lida independente de que fun√ß√£o $f$ estamos tentando generalizar.</li>
</ul>

<p>Agora, utilizando o conceito que acabamos de definir, podemos tentar determinar a dimens√£o VC para algum conjunto conhecido de hip√≥teses $\mathcal{H}$. Por exemplo, para o Perceptron $d$-dimensional, temos que $d_{\text{VC}} = d + 1$. A demonstra√ß√£o desse fato n√£o √© complicada, e o argumento principal recai sobre o fato de que √© poss√≠vel mostrar que: ${}^{(a)}$ $d_{\text{VC}} \leq d + 1$ e ${}^{(b)}$ $d_{\text{VC}} \geq d + 1$. Entretanto, mais importante do que o resultado em si, √© sua interpreta√ß√£o: qual o significado da quantidade $d + 1$ em um modelo Perceptron de $d$ dimens√µes?</p>

<p>Para o algoritmo Perceptron $d$-dimensional, a quantidade $d + 1$ diz respeito √† quantidade de par√¢metros necess√°rios para se determinar a hip√≥tese $h \in \mathcal{H}$. Nesse sentido, podemos dizer que, de alguma forma, a dimens√£o VC √© t√£o maior (ou menor) quanto for o n√∫mero de <strong>par√¢metros efetivos</strong> (aqui, talvez o termo ‚Äú<strong>graus de liberdade</strong>‚Äù seja mais adequado) do modelo com o qual estamos trabalhando.</p>

<p>Al√©m disso, uma outra interpreta√ß√£o da quantidade ‚Äúdimens√£o VC‚Äù pode ser introduzida $-$ nesse caso, uma interpreta√ß√£o <strong>pr√°tica</strong>. Mas antes, vamos relembrar o que a Desigualdade Vapnik-Chervonenkis diz:</p>

<script type="math/tex; mode=display">\begin{align*}
\mathbb{P}\left[ \lvert E_{in}(g) - E_{out}(g) \rvert > \epsilon \right] \leq \delta,
\end{align*}</script>

<p>onde $\delta = 4 \cdot m_{\mathcal{H}}(2N) \cdot e^{-\frac{1}{8} \epsilon^2 N}$.</p>

<p>Da equa√ß√£o anterior, podemos nos perguntar: fixados $\delta$ e $\epsilon$, como se relacionam $N$ e $d_{\text{VC}}$? Para tentar responder √† essa pergunta, ao inv√©s de olharmos para $\delta$ como definido acima, vamos nos concentrar em analisar $\delta = N^{d_{\text{VC}}} \cdot e^{-N}$ (que √© uma simplifica√ß√£o ‚Äúhonesta‚Äù do problema que estamos tentando estudar). O gr√°fico de $\delta(N)$ para diferentes valores de $d_{\text{VC}}$ √© apresentado a seguir.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># plot of (N^d * exp{-N}) for different values of "d"
</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">my_func</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">N</span> <span class="o">**</span> <span class="n">d</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">N</span><span class="p">))</span>

<span class="n">d_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">30</span><span class="p">]</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="n">num</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">d_values</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">my_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"d = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">"black"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"N"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Upper bound for the desired probab. (log scale)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mf">10e-3</span><span class="p">,</span> <span class="mf">10e9</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s">"log"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/dimensao-vc_files/dimensao-vc_9_0.png" alt="png" /></p>

<p>Perceba que a cota superior para a probabilidade deseja tamb√©m cresce √† medida que $d_{\text{VC}}$ cresce. <strong>Observa√ß√£o 1:</strong> apesar de estarmos olhando para uma cota superior de $\mathbb{P}\left[ \lvert E_{in}(g) - E_{out}(g) \rvert &gt; \epsilon \right]$, o comportamento apresentado no gr√°fico traduz, empiricamente, o comportamento da probabilidade desejada. Assim, se estamos trabalhando com modelos de alta dimens√£o VC, precisaremos de uma amostra de tamanho proporcionalmente maior $-$ como regra <strong>pr√°tica</strong>, utilizaremos $N = 10 \cdot d_{\text{VC}}$.</p>

<h3 id="cota-para-teoria-da-generaliza√ß√£o">Cota para Teoria da Generaliza√ß√£o</h3>

<p>Por fim, antes de terminar esse texto, vamos trabalhar um pouco com a Desigualdade de Vapnik-Chervonenkis, reescrevendo-a em um formato que nos ser√° mais √∫til. Dessa forma, se $\delta = 4 \cdot m_{\mathcal{H}}(2N) \cdot e^{-\frac{1}{8} \epsilon^2 N}$, ent√£o:</p>

<script type="math/tex; mode=display">\begin{align*}
\epsilon = \sqrt{\frac{8}{N} \, \ln \left[{\frac{4 \cdot m_{\mathcal{H}}(2N)}{\delta}}\right]} = \Omega(N,\mathcal{H},\delta).
\end{align*}</script>

<p>Assim, com probabilidade $\geq 1 - \delta$,</p>

<script type="math/tex; mode=display">\begin{align*}
\lvert E_{in}(g) - E_{out}(g) \rvert \leq \Omega(N,\mathcal{H},\delta) = \Omega.
\end{align*}</script>

<p>Por√©m, como o termo $E_{in}$ √© (quase sempre) for√ßadamente menor que $E_{out}$ (lembre-se que, quando ajustamos um modelo, estamos tentando minimiar $E_{in}$), podemos simplificar um pouco mais a nossa nota√ß√£o e escrever que, com probabilidade $\geq 1 - \delta$,</p>

<script type="math/tex; mode=display">\begin{align*}
E_{out} - E_{in} \leq \Omega.
\end{align*}</script>

<p>A ineque√ß√£o que acabamos de escrever √© conhecida como uma cota para o ‚Äúerro generalizado‚Äù.</p>

<p>Finalmente, com probabilidade $\geq 1 - \delta$,</p>

<script type="math/tex; mode=display">\begin{align*}
E_{out} \leq E_{in} + \Omega.
\end{align*}</script>

<p>Essa forma de reescrever a Desigualdade de Vapnik-Chervonenkis √© interessante porque, apesar do termo da esquerda permanecer desconhecido, n√≥s temos algum controle sobre $E_{in}$ e $\Omega$ (o primeiro √© o que tentamos minimizar, enquanto que o segundo relaciona-se √† escolha de $\mathcal{H}$). <strong>Observa√ß√£o 2:</strong> aqui, note que um conjunto de hip√≥teses $\mathcal{H}$ ‚Äúgrande‚Äù ajuda a minimar $E_{in}$, mas faz com que $\Omega$ cres√ßa (j√° que, se $\mathcal{H}$ cresce, ent√£o $d_{\text{VC}}$ tamb√©m √© potencialmente maior), o que √© ruim. Dessa forma, deve existir um balan√ßo sobre o tamanho de $\mathcal{H}$ para minimizar o termo $E_{out}$.</p>

<h2 id="conclus√£o">Conclus√£o</h2>

<p>Dando continuidade √† <a href="/teoria-da-generalizacao/">parte 08</a>, definimos e estudamos uma nova quantidade: <strong>dimens√£o VC</strong> ou $d_{\text{VC}}(\mathcal{H})$. Vimos, nesse caso, como o valor de $d_{\text{VC}}$ afeta, do ponto de vista pr√°tico, o tamanho de amostra $N$ que temos que ter para minimar a diferen√ßa entre $E_{out}$ e $E_{in}$. Por √∫ltimo, rearranjando os termos da Desigualdade de Vapnik-Chervonenkis, fomos capazes de escrever uma cota, como fun√ß√£o de $E_{in}$ e de $\Omega(N, \mathcal{H}, \delta)$, para o termo $E_{out}$. No pr√≥ximo post vamos discutir, principalmente, a quest√£o do <em>tradeoff</em> entre vi√©s e vari√¢ncia.</p>

<p>Qualquer d√∫vida, sugest√£o ou <em>feedback</em>, por favor, deixe um coment√°rio abaixo.</p>

<blockquote>
  <p>Essa postagem faz parte de uma <a href="/categorias/#machine-learning-learning-from-data">s√©rie</a> de textos que tem o objetivo de estudar, principalmente, o curso ‚Äú<a href="http://www.work.caltech.edu/telecourse.html">Learning from Data</a>‚Äù, ministrado pelo Professor Yaser Abu-Mostafa. Outros materiais utilizados ser√£o sempre referenciados.</p>
</blockquote>
:ET