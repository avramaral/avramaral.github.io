I"è+<p>Ao longo dessa e da pr√≥xima postagem da nossa <a href="/categorias/#machine-learning-learning-from-data">s√©rie de textos</a>, vamos estabelecer e estudar a distin√ß√£o que tem que existir entre o conjunto de dados utilizados para treinar o nosso modelo, e o conjunto de dados utilizado para test√°-lo.</p>

<p>Nesse sentido, e considerando medidas de erro, o termo <script type="math/tex">E_{out}</script> nos diz o qu√£o bem o nosso modelo ajustado, baseado em <script type="math/tex">\mathcal{D}</script>, generalizou o comportamento da fun√ß√£o alvo (ou distribui√ß√£o alvo) fora desse conjunto de dados <script type="math/tex">(\mathbf{x}_1, y_1), \cdots, (\mathbf{x}_n, y_n)</script> <script type="math/tex">-</script> ou seja, a medida <script type="math/tex">E_{out}</script> √© definida por pontos <script type="math/tex">\mathbf{x} \not\in \mathcal{D}</script>. Em contrapartida, o erro <script type="math/tex">E_{in}</script> baseia-se nos dados de treinamento (conjunto <script type="math/tex">\mathcal{D}</script>). Al√©m disso, como come√ßamos a discutir na <a href="/memorizar-nao-e-aprender/">parte 03</a>, a rela√ß√£o entre <script type="math/tex">E_{in}</script> e <script type="math/tex">E_{out}</script> <script type="math/tex">-</script> dado um conjunto de hip√≥teses <script type="math/tex">\mathcal{H}</script> de tamanho <script type="math/tex">M</script> <script type="math/tex">-</script>, √© definida por:</p>

<script type="math/tex; mode=display">\begin{align*}
\mathbb{P}(\lvert E_{in}(g) - E_{out}(g) \rvert > \epsilon) \leq 2 M e^{-2 \epsilon^2 N}.
\end{align*}</script>

<p>Veja que a inequa√ß√£o acima torna-se pouco √∫til quando consideramos um conjunto $\mathcal{H}$ de tamanho infinito (que √© o que acontece na maior parte dos casos). Lembre-se tamb√©m de que essa cota foi obtida pela propriedade de <em>union bound</em> para a uni√£o dos eventos $\left[ \lvert E_{in}(h_m) - E_{out}(h_m) \rvert &gt; \epsilon \right]$, com $m = 1, \cdots, M$. Entretanto, se existir grande interse√ß√£o entre esses eventos (que √© o que acontece), essa cota n√£o √© boa $-$ vamos, dessa forma, tentar melhor√°-la.</p>

<p>Para alcan√ßar um objetivo como esse, vamos, primeiro, definir uma nova quantidade: a <strong>fun√ß√£o de crescimento</strong>. Essa fun√ß√£o √© o que substituir√° $M$ na nossa cota. A sequ√™ncia de observa√ß√µes e defini√ß√µes a seguir nos levar√° onde queremos chegar.</p>

<p>Agora, ao inv√©s de tomarmos $h: \mathcal{X} \longrightarrow \lbrace -1, +1 \rbrace$ (para simplificar os argumentos, iremos considerar fun√ß√µes avaliadas no conjunto $\lbrace -1, +1 \rbrace$, ao inv√©s de $\mathbb{R}$), vamos restringir o dom√≠nio da fun√ß√£o aos pontos de $\mathcal{D}$; i.e., $h: \lbrace \mathbf{x}_1, \cdots, \mathbf{x}_N \rbrace \longrightarrow \lbrace -1, +1 \rbrace$. Assim, se $h$ for avaliada em uma amostra finita $\lbrace \mathbf{x}_1, \cdots, \mathbf{x}_N \rbrace \subset \mathcal{X}$, iremos obter um $N$-upla $(h(\mathbf{x}_1), \cdots, h(\mathbf{x}_N))$ de $\pm 1 $‚Äôs.Essa ‚Äúnova‚Äù fun√ß√£o $h$ ser√° chamada de <strong>dicotomia</strong>.</p>

<p><strong>Defini√ß√£o 1:</strong> Tome $\lbrace \mathbf{x}_1, \cdots, \mathbf{x}_N \rbrace \subset \mathcal{X}$. As dicotomias geradas por $\mathcal{H}$ sobre esse conjunto de pontos ser√£o definida por:</p>

<script type="math/tex; mode=display">\begin{align*}
\mathcal{H}(\mathbf{x}_1, \cdots, \mathbf{x}_N) = \lbrace (h(\mathbf{x}_1), \cdots, h(\mathbf{x}_N)) \mid h \in \mathcal{H} \rbrace
\end{align*}</script>

<p><strong>Defini√ß√£o 2</strong>: A <strong>fun√ß√£o de crescimento</strong> √© definida, para um conjunto $\mathcal{H}$, como:</p>

<script type="math/tex; mode=display">\begin{align*}
m_{\mathcal{H}}(N) = \max_{\lbrace \mathbf{x}_1, \cdots, \mathbf{x}_N \rbrace \subset \mathcal{X}} \lvert \mathcal{H}(\mathbf{x}_1, \cdots, \mathbf{x}_N) \rvert
\end{align*},</script>

<p>onde $\lvert \cdot \rvert$ √© a cardinalidade do conjunto.</p>

<p>Em palavras, <script type="math/tex">m_{\mathcal{H}}(N)</script> √© n√∫mero m√°ximo de dicotomias que podem ser geradas por <script type="math/tex">\mathcal{H}</script> a partir de um conjunto de <script type="math/tex">N</script> pontos. Aqui, note que, como os elementos de <script type="math/tex">\mathcal{H}(\mathbf{x}_1, \cdots, \mathbf{x}_N)</script> s√£o subconjuntos de <script type="math/tex">\lbrace -1, +1 \rbrace^N</script>, ent√£o <script type="math/tex">m_{\mathcal{H}}(N) \leq 2^N</script>. Al√©m disso, se <script type="math/tex">\mathcal{H}</script> for capaz de gerar todas as poss√≠veis dicotomias sobre <script type="math/tex">\lbrace \mathbf{x}_1, \cdots, \mathbf{x}_N \rbrace</script>, ent√£o <script type="math/tex">\mathcal{H}(\mathbf{x}_1, \cdots, \mathbf{x}_N) = \lbrace -1, +1 \rbrace^N</script> e n√≥s dizemos que <script type="math/tex">\mathcal{H}</script> pode <em>quebrar</em> <script type="math/tex">\lbrace \mathbf{x}_1, \cdots, \mathbf{x}_N \rbrace</script>.</p>

<p>Para entendermos melhor os conceitos que acabamos de definir, considere $\mathcal{H}$ como sendo o conjunto de hip√≥teses associado ao algoritmo Perceptron avaliado em duas dimens√µes. A partir da figura abaixo, podemos dizer que:</p>

<p><img src="/assets/images/teoria-da-generalizacao_files/perceptron-possibilidades.png" alt="Learning from Data" />
<em>Figura 1 [adaptado de: ‚Äú<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>‚Äù] $-$ An√°lise da fun√ß√£o de crescimento para o Perceptron.</em></p>

<p>Olhando para <script type="math/tex">{}^{(a)}</script>, onde os pontos <script type="math/tex">\mathbf{x}_1, \mathbf{x}_2</script> e <script type="math/tex">\mathbf{x}_3</script> foram dispostos de maneira colinear, observe que n√£o conseguimos, utilizando o Perceptron, obter todas as oito <script type="math/tex">3</script>-uplas; entretanto, considerando <script type="math/tex">{}^{(b)}</script>, isso √© poss√≠vel, logo <script type="math/tex">m_{\mathcal{H}}(3) = 8</script>. Por outro lado, para <script type="math/tex">N = 4</script>, somente catorze das poss√≠veis dezesseis <script type="math/tex">4</script>-uplas podem ser geradas pelo Perceptron <script type="math/tex">-</script> assim, <script type="math/tex">m_{\mathcal{H}}(4) = 14</script>.</p>

<p>Veja, nesse caso, que n√£o √© f√°cil calcular, como fun√ß√£o de $N$, a quantidade $m_{\mathcal{H}}(N)$. Observe ainda que, se considerarmos diferentes conjuntos $\mathcal{H}$, associados a diferentes algorimos, essa diculdade pode ser ainda maior. Felizmente, para conseguirmos o que estamos querendo, basta que sejamos capazes de determinar uma cota superior ‚Äúboa‚Äù (veremos a seguir, o que quero dizer por ‚Äúboa‚Äù) para $m_{\mathcal{H}}(N)$. Mas antes disso, mais uma defini√ß√£o:</p>

<p><strong>Defini√ß√£o 3:</strong> se nenhum conjunto de pontos de tamanho $k$ puder ser <em>quebrado</em> (de acordo com a defini√ß√£o que demos ao termo ‚Äúquebrar‚Äù) por $\mathcal{H}$, ent√£o $k$ √© chamado de <strong><em>break point</em></strong> para $\mathcal{H}$.</p>

<p>A partir da Def. 3, √© f√°cil notar que se $\mathcal{H}$ √© a classe de fun√ß√µes associada ao algoritmo Perceptron em duas dimens√µes, ent√£o $k = 4$ √© <em>break point</em> para $\mathcal{H}$.</p>

<p>A Def.3 tamb√©m √© fundamental para conseguirmos a cota ‚Äúboa‚Äù que gostar√≠amos de ter para $m_{\mathcal{H}}(N)$. Veja o teorema a seguir:</p>

<p><strong>Teorema 1:</strong> Se $m_{\mathcal{H}}(N) &lt; 2^k$ para algum valor de $k$; isto √©, se existe <em>break point</em>  $k$ para $\mathcal{H}$, ent√£o</p>

<script type="math/tex; mode=display">\begin{align*}
m_{\mathcal{H}}(N) \leq \sum_{i = 0}^{k - 1} {N \choose i}
\end{align*}</script>

<p>para todo $N$. <strong>Observa√ß√£o:</strong> aqui, o lado direito da inequa√ß√£o √© um polin√¥mio em $N$ de grau $k - 1$.</p>

<p><em>A demonstra√ß√£o do Teorema 1 pode ser encontrada no Cap√≠tulo 2 de <a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>.</em></p>

<p>Observe que um cota como a estabelecida pelo Teor. 1 √© <strong>boa</strong>, pois, se pudermos substituir $M$ por $m_{\mathcal{H}}(N)$ em $\mathbb{P}(\lvert E_{in}(g) - E_{out}(g) \rvert &gt; \epsilon) \leq 2 M e^{-2 \epsilon^2 N}$, ent√£o:</p>

<ol>
  <li>$m_{\mathcal{H}}(N)$ ser√° dominada por alguma coisa que cresce polinomiamente r√°pido com $N$.</li>
  <li>$e^{-2 \epsilon^2 N}$ decresce exponecialmente r√°pido com $N$.</li>
  <li>Logo $\mathbb{P}(\lvert E_{in}(g) - E_{out}(g) \rvert &gt; \epsilon)$  ser√° menor ou igual que alguma quantidade arbitrariamente pr√≥xima de zero, como gostar√≠amos que fosse.</li>
</ol>

<h3 id="desigualdade-de-vapnik-chervonenkis">Desigualdade de Vapnik-Chervonenkis</h3>

<p>Finalmente, o √∫ltimo passo que queremos conseguir justificar √© o da troca de $M$ por $m_{\mathcal{H}}(N)$. Se isso for poss√≠vel, resolvemos nosso problema. Esse resultado (a menos de troca de constantes) √© conhecido como Desigualdade de Vapnik-Chervonenkis e √© enunciado a seguir.</p>

<p><strong>Teorema 2 (Desigualdade de Vapnik-Chervonenkis):</strong> Para qualquer $N \in \mathbb{N}$, vale:</p>

<script type="math/tex; mode=display">\begin{align*}
\mathbb{P}\left[ \lvert E_{in}(g) - E_{out}(g) \rvert > \epsilon \right] \leq 4 \cdot m_{\mathcal{H}}(2N) \cdot e^{-\frac{1}{8} \epsilon^2 N} \text{, } \forall \epsilon > 0.
\end{align*}</script>

<p><em>A demonstra√ß√£o do Teorema 2 pode ser encontrada no Ap√™ndice A de <a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>.</em></p>

<p>No Teor. 2, perceba que as constantes que foram alteradas, em compara√ß√£o √† Desigualdade de Hoeffding, <strong>n√£o</strong> ‚Äújogam ao nosso favor‚Äù. Por√©m, para $N$ suficientemente grande, o lado direito da desigualdade ainda √© arbitrariamente pequeno.</p>

<h2 id="conclus√£o">Conclus√£o</h2>

<p>Como j√° foi estabelecido desde a <a href="/memorizar-nao-e-aprender/">parte 03</a>, nosso interesse √© argumentar que o erro $E_{out}$ (constru√≠do a partir de $\mathcal{X} - \mathcal{D}$) √© bem aproximado por $E_{in}$ (constru√≠do a partir de $\mathcal{D}$). Hav√≠amos resolvido essa quest√£o quando o tamanho de $\mathcal{H}$ √© finito; por√©m, do ponto de vista pr√°tico, esse quase nunca √© o caso. Assim, se pudermos construir uma cota (como na Desigualdade de Hoeffding) que depende de $m_{\mathcal{H}}(N)$ ao inv√©s de $M$, conseguimos generalizar adequamente a ideia de que √© poss√≠vel que o nosso modelo, a partir de um conjunto de hip√≥teses $\mathcal{H}$ com cardinalidade (potencialmente) infinita, aprenda. Felizmente, a desigualdade de Vapnik-Chervonenkis estabelece essa rela√ß√£o e resolve esse problema.</p>

<p>Qualquer d√∫vida, sugest√£o ou <em>feedback</em>, por favor, deixe um coment√°rio abaixo.</p>

<blockquote>
  <p>Essa postagem faz parte de uma <a href="/categorias/#machine-learning-learning-from-data">s√©rie</a> de textos que tem o objetivo de estudar, principalmente, o curso ‚Äú<a href="http://www.work.caltech.edu/telecourse.html">Learning from Data</a>‚Äù, ministrado pelo Professor Yaser Abu-Mostafa. Outros materiais utilizados ser√£o sempre referenciados.</p>
</blockquote>
:ET