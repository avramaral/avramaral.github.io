I"<p>O que nós fizemos até agora foi, dado um conjunto $\mathcal{D}$, treinar um modelo que “explicasse” (no caso do Perceptron, “classificasse”) os dados $(\mathbf{x}_1, y_1), \cdots, (\mathbf{x}_N, y_N)$. Porém, isso significa, de fato, aprender? Isto é, a função $g \in \mathcal{H}$ escolhida pelo algoritmo aproxima a função alvo $f$ no sentido de ter bom desempenho em explicar $\mathbf{x} \not\in \mathcal{D}$?</p>

<p>O que nós vamos fazer agora é introduzir uma componente aleatória no <em>framework</em> de aprendizado que começamos a discutir na <a href="/o-que-e-aprendizado/">parte 01</a> <a href="/categorias/#machine-learning-learning-from-data">dessa série de textos</a> $-$ que vai nos permitir responder “sim” às perguntas do parágrafo anterior.</p>

<p>Para <script type="math/tex">h \in \mathcal{H}</script>, seja <script type="math/tex">\mu = \mathbb{P}\left[h(\mathbf{x}_n) \neq f(\mathbf{x}_n)\right]</script>, com <script type="math/tex">n = 1, \cdots, N</script>; i.e., a probabilidade de que o valor de uma função fixa <script type="math/tex">h</script>  avaliada em <script type="math/tex">\mathbf{x}_n</script> <script type="math/tex">-</script> com <script type="math/tex">h</script> escolhida antes de <script type="math/tex">\mathcal{D}</script> ser gerado <script type="math/tex">-</script> seja diferente da função alvo $f$ avaliada no mesmo ponto. Por consequência, <script type="math/tex">1 - \mu = \mathbb{P}\left[h(\mathbf{x}_n) = f(\mathbf{x}_n)\right]</script>. Além disso, defina <script type="math/tex">\nu = \frac{1}{N} \sum_{n= 1}^{N} \mathbb{I}_{\lbrace h(\mathbf{x}_n) \neq f(\mathbf{x}_n)\rbrace}</script>; ou seja, <script type="math/tex">\nu</script> é a proporção de vezes que <script type="math/tex">h(\mathbf{x}_n)</script> é diferente de <script type="math/tex">f(\mathbf{x}_n)</script> para uma amostra <script type="math/tex">\mathcal{D}</script>. A ideia é que, desde que a <script type="math/tex">\mathcal{D}</script> seja gerada aleatoriamente seguindo uma distribuição <script type="math/tex">P</script> (não necessariamente conhecida), então <script type="math/tex">\nu</script> aproxima bem <script type="math/tex">\mu</script>. A relação a seguir, conhecida como <em>Desigualdade de Hoeffding</em>, quantifica essa aproximação:</p>

<script type="math/tex; mode=display">\begin{align*}
    \mathbb{P}\left[\lvert \nu - \mu\rvert > \epsilon\right] \leq 2e^{-2 \epsilon^2 N} \text{, para todo N e }\forall \epsilon > 0.
\end{align*}</script>

<p>O que a inequação acima diz é que a probabilidade de $\nu$ estar arbitrariamente próximo de $\mu$ é algo como “$1$ menos alguma coisa que decai exponencialmente com $N$”. O que é o mesmo que dizer que, para $N$ “grande”, $\nu$ aproxima bem o comportamento de $\mu$. Assim, a quantidade de vezes que $h$ erra na amostra $\mathcal{D}$ é proporcional à quantidade de erros que $h$ cometeria fora de $\mathcal{D}$. Veja abaixo um esquema atualizado das nossas componentes do aprendizado.</p>

<p><img src="/assets/images/memorizar-nao-e-aprender_files/comp-aprendiz-estocastico.png" alt="Framework de aprendizado com componente estocástica" />
<em>Figura 1 [fonte: “<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>”] $-$ Framework de aprendizado atualizado com componente estocástica.</em></p>

<p>Dito tudo isso, se houvesse apenas uma função em $\mathcal{H}$, seria fácil de verificar se $h$ tem bom desempenho em avaliar pontos fora de $\mathcal{D}$; só que esse não é o caso $-$ na maior parte das vezes, $\mathcal{H}$ tem cardinalidade infinita, inclusive. Isso nos motiva a introduzir uma nova notação:</p>

<p>Defina <script type="math/tex">E_{in}(h) = \frac{1}{N} \sum_{n= 1}^{N}\mathbb{I}_{\lbrace h(\mathbf{x}_n) \neq f(\mathbf{x}_n)\rbrace}</script> e <script type="math/tex">E_{out}(h) = \mathbb{P}\left[h(\mathbf{x}_n) \neq f(\mathbf{x}_n)\right]</script>; ou seja, <script type="math/tex">E_{in}(h)</script> e <script type="math/tex">E_{out}(h)</script> são, respectivamente, as quantidades <script type="math/tex">\nu</script> e <script type="math/tex">\mu</script> <strong>como função de <script type="math/tex">h</script></strong>. Então é óbvio que, para todo <script type="math/tex">N</script>, <script type="math/tex">\mathbb{P}\left[\lvert E_{in}(h) - E_{out}(h)\rvert > \epsilon\right] \leq 2e^{-2 \epsilon^2 N}</script>, <script type="math/tex">\forall \epsilon > 0</script>. O ganho em definir essa nova notação aparece no próximo parágrafo. <strong>Observação:</strong> o subscrito “<script type="math/tex">in</script>” faz referência ao termo <em>in-sample</em>; da mesma forma, “<script type="math/tex">out</script>” quer dizer <em>out-of-sample</em>.</p>

<p>A cota que temos até agora diz respeito a uma única função $h \in \mathcal{H}$ ; porém, se $\mathcal{H}$ tem cardinalidade maior que $1$ (o que, na prática, é sempre verdade), podemos escrever uma relação parecida para uma função $g \in \mathcal{H}$ escolhida por $\mathcal{A}$. Seja $\mathcal{H}$ conjunto finito de tamanho $M$, então vale:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\mathbb{P}\left[\lvert E_{in}(g) - E_{out}(g)\rvert > \epsilon\right] & \leq \mathbb{P}\left[\bigcup_{m = 1}^{M} \left[ \lvert E_{in}(h_m) - E_{out}(h_m)\rvert > \epsilon \right] \right] \\
& \leq \sum_{m = 1}^{M} \mathbb{P}\left[\lvert E_{in}(h_m) - E_{out}(h_m)\rvert > \epsilon \right] \\
& \leq \sum_{m = 1}^{M} 2e^{-2 \epsilon^2 N} = 2 M e^{-2 \epsilon^2 N}.
\end{align*} %]]></script>

<p>Nas equivalências acima, a primeira desigualdade é justificada por inclusão de eventos, a segunda por <em>union bound</em>, e a terceira, como já vimos, vem da Desigualdade de Hoeffding.</p>

<p>A princípio, essa cota que conseguimos para $g$ só faz sentido se $M$ for finito; já que o lado direito da desigualdade cresce com $M$. Entretanto, esse resultado pode ser generalizado para $\mathcal{H}$ conjunto infinito.</p>

<p>Em resumo, é possível interpretar o resultado de que $\mathbb{P}\left[\lvert E_{in}(g) - E_{out}(g)\rvert &gt; \epsilon\right] \leq 2 M e^{-2 \epsilon^2 N}$ da seguinte forma: a depender de $M$ e $\epsilon$, o nosso modelo consegue, de fato, <strong>aprender</strong>, pois a função $g \in \mathcal{H}$ escolhida por $\mathcal{A}$ se aproxima de $f$ quando $N$ cresce $-$ no sentido de, se $E_{in}(g) \approx 0$, ter probabilidade de erro arbitrariamente pequena para avaliar $\mathbf{x} \not\in \mathcal{D}$.</p>

<h2 id="conclusão">Conclusão</h2>

<p>Vimos que, com a introdução de uma componente estocástica no nosso <em>framework</em> de aprendizado, é possível que nosso modelo aprenda (e não apenas memorize). Nesse caso, quando $N$ cresce, $E_{in}(g) \approx E_{out}(g)$. Assim, se conseguirmos fazer com que $E_{in}(g) \approx 0$, então teremos que $E_{out}(g) \approx 0$; o que é o mesmo que dizer que se o algoritmo $\mathcal{A}$ conseguir escolher uma função $g \in \mathcal{H}$ que tem bom desempenho em avaliar $\mathbf{x} \in \mathcal{D}$, então $g$ também terá bons resultados para observações fora de $\mathcal{D}$.</p>

<p>Qualquer dúvida, sugestão ou <em>feedback</em>, por favor, deixe um comentário abaixo.</p>

<blockquote>
  <p>Essa postagem faz parte de uma <a href="/categorias/#machine-learning-learning-from-data">série</a> de textos que tem o objetivo de estudar, principalmente, o curso “<a href="http://www.work.caltech.edu/telecourse.html">Learning from Data</a>”, ministrado pelo Professor Yaser Abu-Mostafa. Outros materiais utilizados serão sempre referenciados.</p>
</blockquote>
:ET