I"®<p>Voltando a falar de modelos lineares, como os de classifica√ß√£o $-$ explorados nas partes <a href="/implementacao-perceptron/">02</a> e <a href="/modelos-lineares-de-classificacao-e-pocket/">04</a> $-$ ou de regress√£o linear, a exemplo do que vimos na <a href="/modelo-de-regressao-linear/">parte 05</a>, iremos discutir nesse post o que √© e como funciona a regress√£o log√≠stica. Veremos que esse novo modelo herda caracter√≠sticas das duas classes de algoritmos que j√° estudamos, j√° que √© a fun√ß√£o alvo √© real-avaliada (como na regress√£o linear), mas √© limitada (como no Perceptron ou Pocket, por exemplo).</p>

<p>Na regress√£o log√≠stica, temos que $f(x) \in [0,1]$; e, como veremos a seguir, esse valor ser√° interpretado como uma probabilidade. Para termos contexto, considere o seguinte exemplo: suponha que voc√™ quer modelar, com base em caracter√≠ticas como idade, peso, n√≠vel de colesterol, etc., a probabilidade de um indiv√≠duo sofrer um ataque card√≠aco nos pr√≥ximos 12 meses. De posse desse cen√°rio, vamos come√ßar a estabelecer as quantidades de interesse.</p>

<p>Primeiro, vamos come√ßar definindo nossa classe de fun√ß√µes $\mathcal{H}$ para um problema de regress√£o log√≠stica. Nesse caso, teremos que</p>

<script type="math/tex; mode=display">\begin{align*}
h(\mathbf{x}) = \theta(\mathbf{w}^{\text{T}} \mathbf{x}),
\end{align*}</script>

<p>onde $\theta(s) = \frac{e^s}{1 + e^s}$. Para enxergar como se comporta a fun√ß√£o $\theta$, veja o gr√°fico a seguir.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">theta_func</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">num</span> <span class="o">=</span> <span class="mi">120</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">theta_func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s">"--"</span><span class="p">,</span>  <span class="n">color</span> <span class="o">=</span> <span class="s">"black"</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s">"--"</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">"black"</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s">"--"</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">"black"</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"s"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"$</span><span class="err">\</span><span class="s">Theta$(s)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/modelo-de-regressao-logistica_files/modelo-de-regressao-logistica_3_0.png" alt="png" /></p>

<p>Agora, podemos nos concentrar em analisar a fun√ß√£o alvo que a regress√£o log√≠stica est√° tentando aprender. Logo,</p>

<script type="math/tex; mode=display">\begin{align*}
f(\mathbf{x}) = \mathbb{P}[y = +1 \mid \mathbf{x}].
\end{align*}</script>

<p>Por√©m, perceba que, utilizando $\mathcal{D}$, n√£o temos valores de $f(\mathbf{x})$, mas sim de amostras geradas a partir dessa distribui√ß√£o de probabilidade (no nosso exemplo, n√£o sabemos dizer a probabilidade com que um paciente sofreu um ataque card√≠aco; nesse caso, s√≥ sabemos que esse incidente aconteceu). Por isso, estaremos interessados em trabalhar com, ao inv√©s da fun√ß√£o alvo, a <strong>distribui√ß√£o alvo</strong>:</p>

<script type="math/tex; mode=display">% <![CDATA[
P(y \mid \mathbf{x}) = 
\begin{cases}  
    f(\mathbf{x}) & \text{ para } y = +1 \\
    1 - f(\mathbf{x}) & \text{ para } y = -1.
\end{cases} %]]></script>

<p>Finalmente, s√≥ nos resta definir uma medida de erro $\text{e}(h(\mathbf{x}), y)$ para minimizar. Aqui, a ideia de verossimilhan√ßa ir√° exercer papel fundamental $-$ ou seja, o processo ser√° o de encontrar a $h \in \mathcal{H}$ que, baseado em $\mathbf{x}$, melhor explica o resultado $y$. Assim, a verossimilhan√ßa ser√° dada por:</p>

<script type="math/tex; mode=display">% <![CDATA[
P(y \mid \mathbf{x}; \mathbf{w}) = 
\begin{cases}  
    h_{\mathbf{w}}(\mathbf{x}) & \text{ para } y = +1 \\
    1 - h_{\mathbf{w}}(\mathbf{x}) & \text{ para } y = -1.
\end{cases} %]]></script>

<p>Ent√£o, substituindo $h_{\mathbf{w}}(\mathbf{x})$ por $\theta(\mathbf{x}^{\text{T}} \mathbf{x})$ e utilizando o fato de que $\theta(-s) = 1 - \theta(s)$, temos que:</p>

<script type="math/tex; mode=display">\begin{align*}
P(y \mid \mathbf{x}; \mathbf{w}) = \theta(y \; \mathbf{w}^{\text{T}}\mathbf{x}).
\end{align*}</script>

<p>J√° que, por hip√≥tese, os pontos de $\mathcal{D}$ s√£o gerados de maneira independente, ent√£o:</p>

<script type="math/tex; mode=display">\begin{align*}
\prod_{n = 1}^{N} P(y_n \mid \mathbf{x}_n; \mathbf{w}) = \prod_{n = 1}^{N} \theta(y_n \; \mathbf{w}^{\text{T}}\mathbf{x}_n).
\end{align*}</script>

<p>Como maximizar a fun√ß√£o de verossimilhan√ßa √© o mesmo que minizar o erro, podemos definir $E_{in}$ como:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
E_{in}(\mathbf{w}) & = -\frac{1}{N} \; \ln \left[ \prod_{n = 1}^{N} \theta(y_n \; \mathbf{w}^{\text{T}}\mathbf{x}_n) \right] \\
& = \frac{1}{N} \sum_{n = 1}^{N} \ln \left[ \frac{1}{\theta(y_n \; \mathbf{w}^{\text{T}}\mathbf{x}_n)} \right] \\
& = \frac{1}{N} \sum_{n = 1}^{N} \ln \left[ 1 + e^{- y_n \mathbf{w}^{\text{T}}\mathbf{x}_n} \right] \text{, tal que } \theta(s) = \frac{1}{1 + e^{-s}}
\end{align*} %]]></script>

<p>De forma explicita, acabamos de dizer que $\text{e}(h(\mathbf{x}_n), y_n)= \left[ 1 + e^{- y_n \mathbf{w}^{\text{T}}\mathbf{x}_n} \right]$. A medida de erro definida dessa maneira √© chamada de <em>‚Äòcross entropy‚Äô error</em>.</p>

<p>Agora, a nossa miss√£o √© encontrar $\mathbf{w}$ que minimiza $E_{in}$; por√©m, da maneira como definimos o erro, n√£o √© poss√≠vel determinar uma solu√ß√£o anal√≠tica para esse problema. Assim, utilizaremos uma t√©cnica (de itera√ß√£o) conhecida como <strong>Gradiente Descendente</strong>.</p>

<h3 id="gradiente-descendente-para-regress√£o-log√≠stica">Gradiente Descendente para Regress√£o Log√≠stica</h3>

<p>Gradiente descendente √© um algoritmo utilizado para minimizar fun√ß√µes que s√£o, pelo menos, duas vezes diferenci√°veis, tal como $E_{in}(\mathbf{w})$. A ideia √© que, atrav√©s de um processo de itera√ß√£o, o algoritmo, que come√ßa com $\mathbf{w}(0)$ definido arbitrariamanente, encontra $\mathbf{w}(m)$ $-$ para $m$ suficientemente grande $-$, que √© <em>m√≠nimo local</em> de $E_{in}(\mathbf{w})$. Entretanto, o fato de n√£o existir garantia de que o <em>m√≠nimo local</em> √©, tamb√©m, <em>m√≠nimo global</em>, √© um problema que precisa ser tratato (esse cen√°rio ser√° discutido em outra oportunidade). Felizmente, para a regress√£o log√≠stica (bem como para o modelo de regress√£o linear), $E_{in}$ √© fun√ß√£o convexa, o que implica que, nesse caso, o ponto de m√≠nimo √© √∫nico.</p>

<p>Dito isso, vamos descrever como encontrar esse ponto de m√≠nimo. Aqui, suponha que vamos dar pequenos passos de tamanho $\eta$ na dire√ß√£o do vetor unit√°rio $\mathbf{\hat{v}}$; assim, teremos $\mathbf{w}(1) = \mathbf{w}(0) + \eta \; \mathbf{\hat{v}}$. Dessa forma, com a inten√ß√£o de fazer $\Delta E_{in}$ o menor poss√≠vel, obtemos, utilizando a expans√£o de Taylor (<a href="https://en.wikipedia.org/wiki/Taylor_series#Taylor_series_in_several_variables">ref.</a>):</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\Delta E_{in} & = E_{in}(\mathbf{w}(0) + \eta \mathbf{\hat{v}}) - E_{in}(\mathbf{w}(0)) \\
& = E_{in}(\mathbf{w}(0)) + (\mathbf{w}(1) - \mathbf{w}(0))^{\text{T}} \; \nabla E_{in}(\mathbf{w}(0)) + \mathcal{O}(\eta^2) - E_{in}(\mathbf{w}(0)) \\
& \geq (\eta \; \mathbf{\hat{v}})^{\text{T}} \; \nabla E_{in}(\mathbf{w}(0)) \\
& = \eta \; \nabla E_{in}(\mathbf{w}(0))^{\text{T}} \; \mathbf{\hat{v}} \\
& \geq - \eta \; \lvert \lvert \nabla E_{in}(\mathbf{w}(0)) \rvert \rvert \text{, j√° que } \max_{u: \lvert \lvert u \rvert \rvert = 1} \langle u, v \rangle = \lvert \lvert v \rvert \rvert.
\end{align*} %]]></script>

<p>Assim, em rela√ß√£o a √∫ltima desigualdade, vale o ‚Äú$=$‚Äù se e somente se:</p>

<script type="math/tex; mode=display">\begin{align*}
\mathbf{\hat{v}} = -\frac{\nabla E_{in}(\mathbf{w}(0))}{\lvert \lvert \nabla E_{in}(\mathbf{w}(0)) \rvert \rvert}.
\end{align*}</script>

<p>Ou seja, o vetor unit√°rio $\mathbf{\hat{v}}$ com dire√ß√£o como definido acima, √© o que me d√° a maior varia√ß√£o negativa de $\Delta E_{in}$ para um passo de tamanho $\eta$. A ideia, a partir desse ponto, √© iterar sobre o processo que acabamos de descrever.</p>

<p>Por√©m, um problema surge: como podemos definir adequadamente o tamanho do passo $\eta$? Se ele for muito pequeno, como na imagem da esquerda (Fig. 1), o algoritmo √© ineficiente quando n√£o estamos perto do m√≠nimo local; se ele for muito grande, como na imagem do centro, podemos, inclusive, aumentar $E_{in}$. Nesse caso, o ideal √© tomar passos de tamanho proporcional √† dist√¢ncia que estamos do ponto de m√≠nimo, como na imagem da direita.</p>

<p><img src="/assets/images/modelo-de-regressao-logistica_files/tamanho-de-eta.png" alt="Tamanho de eta" />
<em>Figura 1 [fonte: ‚Äú<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>‚Äù] $-$ Op√ß√µes para tamanho de $\eta$.</em></p>

<p>Para alcan√ßar uma solu√ß√£o como a apresentada na imagem da direita (Fig. 1) basta fazer $\eta = \eta \; \lvert\lvert \nabla E_{in} \rvert\rvert$. Essa estrat√©gia funciona pois, longe do m√≠nimo local, a norma vetor gradiente √© tipicamente maior; ao passo que, perto do ponto de m√≠nimo, essa quantidade diminui.</p>

<p>Por fim, como</p>

<script type="math/tex; mode=display">\nabla E_{in} = - \frac{1}{N} \sum_{n = 1}^{N} \frac{y_n \mathbf{x}_n}{1 + e^{y_n \mathbf{w}^{\text{T}}\mathbf{x}_n}},</script>

<p>somos capazes de implementar um algoritmo, como explicitado na Fig. 2, para resolver o problema de regress√£o log√≠stica.</p>

<p><img src="/assets/images/modelo-de-regressao-logistica_files/algoritmo-reg-log.png" alt="Algoritmo Regress√£o Log√≠stica" />
<em>Figura 2 [fonte: ‚Äú<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>‚Äù] $-$ Algoritmo para Regress√£o Log√≠stica.</em></p>

<h3 id="implementa√ß√£o-em-python">Implementa√ß√£o em Python</h3>

<p>Vamos come√ßar por, seguindo as ideias que desenvolvemos ao longo do texto e o algoritmo apresentado na Fig. 2, implementar a classe <code class="highlighter-rouge">LogisticRegression</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Raw implementation
</span><span class="k">class</span> <span class="nc">LogisticRegression</span><span class="p">:</span>
    <span class="s">"""
    Logist Regression Algorithm
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">prob_threshold</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">prob_out</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">eta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prob_out</span> <span class="o">=</span> <span class="n">prob_out</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prob_threshold</span> <span class="o">=</span> <span class="n">prob_threshold</span>
    
    <span class="k">def</span> <span class="nf">sigmoid_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">gradient_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">partial_sum</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">X</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">X_i</span><span class="p">,</span> <span class="n">y_i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
            <span class="n">num</span> <span class="o">=</span> <span class="n">y_i</span> <span class="o">*</span> <span class="n">X_i</span> 
            <span class="n">den</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">y_i</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">))))</span>
            <span class="n">partial_sum</span> <span class="o">+=</span> <span class="p">(</span><span class="n">num</span> <span class="o">/</span> <span class="n">den</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">/</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">partial_sum</span> 
            
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">):</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w_</span> <span class="o">-=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">grad</span><span class="p">)</span> 
        <span class="k">return</span> <span class="bp">self</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_i</span><span class="p">):</span>
        <span class="n">probability</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid_</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">w_</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prob_out</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">probability</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">probability</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prob_threshold</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Adotando o mesmo padr√£o de nomes para outras classes que j√° escrevemos, o m√©todo <code class="highlighter-rouge">fit()</code> ajusta o modelo $-$ utilizando, para isso, a fun√ß√£o <code class="highlighter-rouge">gradient_()</code>. O m√©todo <code class="highlighter-rouge">predict()</code>, atrav√©s da fun√ß√£o <code class="highlighter-rouge">sigmoid_()</code>, calcula a probabilidade de uma nova observa√ß√£o assumir valor $+1$ ou, como usaremos a partir desse daqui, classifica um novo ponto de acordo com o limite <code class="highlighter-rouge">prob_threshold</code>.</p>

<p>Agora, vamos, com um conjunto de dados simulado, ajustar o modelo. Mas primeiro, veja como os pontos s√£o distribu√≠dos.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span> <span class="o">=</span> <span class="mi">250</span>

<span class="n">rn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">999</span><span class="p">)</span>
<span class="n">c1</span> <span class="o">=</span> <span class="n">rn</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">N</span><span class="p">)</span>
<span class="n">c2</span> <span class="o">=</span> <span class="n">rn</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">N</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">c1</span><span class="p">,</span> <span class="n">c2</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">)]),</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">)])]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="n">N</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s">"red"</span><span class="p">,</span>   <span class="n">marker</span> <span class="o">=</span> <span class="s">"o"</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"Class A"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">N</span><span class="p">:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">N</span><span class="p">:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span> <span class="o">=</span> <span class="s">"green"</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">"s"</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"Class B"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/modelo-de-regressao-logistica_files/modelo-de-regressao-logistica_26_0.png" alt="png" /></p>

<p>Perceba que as duas classes <code class="highlighter-rouge">Class A</code> e <code class="highlighter-rouge">Class B</code> <strong>n√£o</strong> s√£o linearmente separ√°veis; mais do que isso, na verdade: a regi√£o de interse√ß√£o entre as duas nuvens de pontos √© razoavelmente grande. Abaixo, o ajuste do modelo:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my_logReg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">my_logReg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">my_logReg</span><span class="o">.</span><span class="n">w_</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[-1.1072962  -0.52486136  1.16919589]
</code></pre></div></div>

<p>A seguir, vamos plotar as regi√µes de decis√£o para cada uma das classes com <code class="highlighter-rouge">prob_threshold = 0.5</code>. Para isso, utilizaremos a nossa fun√ß√£o, j√° v√°rias vezes mencionada ao longo dos posts, <code class="highlighter-rouge">plot_decision_regions()</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classifier</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">resolution</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">plot_lim</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">):</span>
    <span class="c1"># general settings
</span>    <span class="n">markers</span> <span class="o">=</span> <span class="p">[</span><span class="s">"o"</span><span class="p">,</span> <span class="s">"s"</span><span class="p">,</span> <span class="s">"*"</span><span class="p">,</span> <span class="s">"x"</span><span class="p">,</span> <span class="s">"v"</span><span class="p">]</span>
    <span class="n">colors</span>  <span class="o">=</span> <span class="p">(</span><span class="s">"red"</span><span class="p">,</span> <span class="s">"green"</span><span class="p">,</span> <span class="s">"blue"</span><span class="p">,</span> <span class="s">"gray"</span><span class="p">,</span> <span class="s">"cyan"</span><span class="p">)</span>
    <span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">plot_lim</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">plot_lim</span>
    <span class="n">x2_min</span><span class="p">,</span> <span class="n">x2_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">()</span> <span class="o">-</span> <span class="n">plot_lim</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">plot_lim</span>
    <span class="c1"># define a grid
</span>    <span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x1_min</span><span class="p">,</span> <span class="n">x1_max</span><span class="p">,</span> <span class="n">resolution</span><span class="p">),</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x2_min</span><span class="p">,</span> <span class="n">x2_max</span><span class="p">,</span> <span class="n">resolution</span><span class="p">))</span>
    <span class="c1"># classify each grid point
</span>    <span class="n">result</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">xx1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">xx2</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="c1"># make a plot
</span>    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx1</span><span class="p">,</span> <span class="n">xx2</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">colors</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))],</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span> 
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">value</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">value</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> 
                    <span class="n">color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
                    <span class="n">marker</span> <span class="o">=</span> <span class="n">markers</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
                    <span class="n">label</span> <span class="o">=</span> <span class="n">feature_names</span><span class="p">[</span><span class="n">index</span><span class="p">],</span>
                    <span class="n">edgecolor</span> <span class="o">=</span> <span class="s">'black'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Classe A'</span><span class="p">,</span> <span class="s">'Classe B'</span><span class="p">]</span>
<span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">my_logReg</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">plot_lim</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Fitted Model with Raw Implementation of Logistic Regression"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"$x_1$"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"$x_2$"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/modelo-de-regressao-logistica_files/modelo-de-regressao-logistica_31_0.png" alt="png" /></p>

<p>No gr√°fico acima, veja que conseguimos, √† medida do que √© poss√≠vel com o conjunto de dados de treinamento $\mathcal{D}$, criar regi√µes que classificam corretamente a maior parte dos pontos.</p>

<p>Entretanto, abordando o problema dessa maneira $-$ como um problema de classifica√ß√£o simples $-$, perdemos uma caracter√≠stica importante do modelo de regress√£o log√≠stica: a ideia de que cada ponto vale $+1$ com <strong>probabilidade</strong> $h_{\mathbf{w}}(\mathbf{x})$ (na verdade, $g_{\mathbf{w}}(\mathbf{x})$ $-$ se $g \in \mathcal{H}$ foi escholida por $\mathcal{A}$). Veja o trecho de c√≥digo abaixo:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Test: predict the probability, instead of the class
</span><span class="n">my_logReg</span><span class="o">.</span><span class="n">prob_out</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">chosen_point</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># a very 'difficult' point to classify
</span><span class="k">print</span><span class="p">(</span><span class="s">"With probability {:.2f}, the point {} is evaluated as +1."</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">my_logReg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">chosen_point</span><span class="p">),</span> <span class="n">chosen_point</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>With probability 0.52, the point [0 1] is evaluated as +1.
</code></pre></div></div>

<p>Observe que o ponto $(0, 1)$, analisando as regi√µes de decis√£o que foram plotadas, est√° no limiar da fronteira de classifica√ß√£o. O que faz sentido com o fato de que obtivemos $\mathbb{P}[y = +1 \mid \mathbf{x}] = 0.52$; ou seja, apesar de $(0, 1)$ ter sido classificado como $+1$ (para o threshold de $0.5$), isso aconteceu por bem pouco.</p>

<p>Para finalizar, assim como j√° fizemos outras vezes, vamos utilizar a implementa√ß√£o do algoritmo pela biblioteca Sklearn.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="n">sk_logReg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">sk_logReg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sk_logReg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sk_logReg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[-4.5329416]
[[-1.70651758  3.08848517]]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Class A'</span><span class="p">,</span> <span class="s">'Class B'</span><span class="p">]</span>
<span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sk_logReg</span><span class="p">,</span> <span class="n">feature_names</span><span class="p">,</span> <span class="n">plot_lim</span> <span class="o">=</span> <span class="mf">0.15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Fitted model with Sklearn version of Logistic Regression"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"$x_1$"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"$x_2$"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/modelo-de-regressao-logistica_files/modelo-de-regressao-logistica_38_0.png" alt="png" /></p>

<p>Como era de se espera, a utiliza√ß√£o da classe <code class="highlighter-rouge">LogisticRegression</code> √© simples e apresenta resultados semelhantes aos que obtivemos acima. A performance nesse segundo caso √©, obviamente, muito mais otimizada; por√©m, como forma de aprendizado, a nossa classe cumpriu bem a tarefa.</p>

<h2 id="conclus√£o">Conclus√£o</h2>

<p>Complementando os modelos lineares (e transforma√ß√µes n√£o-lineares) que j√° hav√≠amos come√ßado a estudar nas partes <a href="/modelo-de-regressao-linear/">05</a> e <a href="/transformacoes-nao-lineares/">06</a>, vimos como o modelo de Regress√£o Log√≠stica pode ser utilizado para, por exemplo, problemas de classifica√ß√£o. Entretando, a sua capacidade de fazer esse tipo de an√°lise do ponto de vista probabil√≠stico √© o que √© o mais interessante. Implementamos o algoritmo em Python e, em adi√ß√£o √†s classes que escrevemos antes, estamos come√ßando a construir um bom reposit√≥rio de algoritmos de <em>machine learning</em>. O pr√≥ximo post discutir√° o assunto ‚Äú<em>neural networks</em>‚Äù (ou ‚Äúredes neurais‚Äù).</p>

<p>Qualquer d√∫vida, sugest√£o ou <em>feedback</em>, por favor, deixe um coment√°rio abaixo.</p>

<blockquote>
  <p>Essa postagem faz parte de uma <a href="/categorias/#machine-learning-learning-from-data">s√©rie</a> de textos que tem o objetivo de estudar, principalmente, o curso ‚Äú<a href="http://www.work.caltech.edu/telecourse.html">Learning from Data</a>‚Äù, ministrado pelo Professor Yaser Abu-Mostafa. Outros materiais utilizados ser√£o sempre referenciados.</p>
</blockquote>
:ET