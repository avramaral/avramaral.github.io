<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.17.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="pt" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>[Parte 12] Redes Neurais - Blog do André</title>
<meta name="description" content="Ao longo desse post vamos, como temos feito para todos os modelos que discutimos até agora nessa série de textos, estudar o que são, do ponto de vista mais teórico, as redes neurais. A ideia é que, na próxima postagem, a gente consiga implementar em Python o que vamos estudar a partir desse momento.">


  <meta name="author" content="André V. R. Amaral">


<meta property="og:type" content="article">
<meta property="og:locale" content="pt_BR">
<meta property="og:site_name" content="Blog do André">
<meta property="og:title" content="[Parte 12] Redes Neurais">
<meta property="og:url" content="http://localhost:4000/redes-neurais/">


  <meta property="og:description" content="Ao longo desse post vamos, como temos feito para todos os modelos que discutimos até agora nessa série de textos, estudar o que são, do ponto de vista mais teórico, as redes neurais. A ideia é que, na próxima postagem, a gente consiga implementar em Python o que vamos estudar a partir desse momento.">







  <meta property="article:published_time" content="2020-01-28T00:00:00-03:00">





  

  


<link rel="canonical" href="http://localhost:4000/redes-neurais/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "André",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Blog do André Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<link rel="shortcut icon" href="../../assets/images/favicon.ico" type="image/x-icon">

<!-- end custom head snippets -->


    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
    </script>
    
  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Blog do André
          <span class="site-subtitle">Probabilidade, estatística e mais.</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/categorias/" >Categorias</a>
            </li><li class="masthead__menu-item">
              <a href="/busca/" >Busca</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/profile_picture.jpeg" alt="André V. R. Amaral" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">André V. R. Amaral</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Mestrando em Estatística pela <a href="http://www.est.ufmg.br/portal/">Universidade Federal de Minas Gerais</a>.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Informações</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="/sobre/" rel="nofollow noopener noreferrer"><i class="far fa-fw fa-question-circle" aria-hidden="true"></i> Sobre</a></li>
          
        
          
            <li><a href="mailto:avramaral@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> E-mail</a></li>
          
        
          
            <li><a href="https://github.com/avramaral" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          
        
          
            <li><a href="https://twitter.com/avramaral" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i> Twitter</a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/andre-victor-ribeiro-amaral/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin-in" aria-hidden="true"></i> LinkedIn</a></li>
          
        
          
            <li><a href="https://www.youtube.com/channel/UC5RVscKF68DWYixtJPrBccQ" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-youtube" aria-hidden="true"></i> YouTube</a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="[Parte 12] Redes Neurais">
    <meta itemprop="description" content="Ao longo desse post vamos, como temos feito para todos os modelos que discutimos até agora nessa série de textos, estudar o que são, do ponto de vista mais teórico, as redes neurais. A ideia é que, na próxima postagem, a gente consiga implementar em Python o que vamos estudar a partir desse momento.">
    <meta itemprop="datePublished" content="January 28, 2020">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">[Parte 12] Redes Neurais
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  14 minuto(s) de leitura

</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>Ao longo desse post vamos, como temos feito para todos os modelos que discutimos até agora <a href="/categorias/#machine-learning-learning-from-data">nessa série de textos</a>, estudar o que são, do ponto de vista mais teórico, as redes neurais. A ideia é que, na próxima postagem, a gente consiga implementar em Python o que vamos estudar a partir desse momento.</p>

<p>Para começar, podemos tentar definir o que são as <strong>redes neurais</strong>. Bem, há várias analogias sobre como o modelo matemático que recebe o nome de “rede neural” se compara à forma através da qual o ser humano aprende, etc., etc. Mas, grosso modo, redes neurais são modelos, compostos por pequenas “peças”, que têm o objetivo de aprender (no sentido que temos estudado) funções mais complexas. Essas “peças” podem, em essência, ser construídas a partir de qualquer transformação não-linear. Porém, o caso mais comum é trabalharmos com blocos que utilizam funções do tipo “<em>s-shaped curves</em>”. A imagem abaixo ilustra uma combinação desse tipo.</p>

<p><img src="/assets/images/redes-neurais/rede-neural.png" alt="Rede Neural" />
<em>Figura 1 [fonte: “<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>”] $-$ Rede Neural.</em></p>

<p>A partir da Fig 1., podemos identificar algumas componentes. Em primeiro lugar, cada coluna de “nós” (ou “<em>nodes</em>”) é chamada de “<em>layer</em>” (ou “camada”); à primeira camada, vamos dar o nome de “<em>input</em>”, e à última, “<em>output</em>” $-$ as camadas intermediárias vão ser chamadas de “<em>hidden layers</em>”. A função de ativação $\theta$ será definida a partir da “tangente hiperbólica”; i.e., $\theta(s) = \tanh(s) = \frac{e^s - e^{-s}}{e^s + e^{-s}}$. A imagem abaixo ilustra o comportamento de $\theta(s)$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">theta_func</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">s</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">s</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">num</span> <span class="o">=</span> <span class="mi">120</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">theta_func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s">"--"</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">"black"</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s">"--"</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">"black"</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s">"--"</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">"black"</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span> <span class="mi">1</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s">"--"</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">"black"</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"s"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"$</span><span class="err">\</span><span class="s">Theta$(s)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/assets/images/redes-neurais/redes-neurais_4_0.png" alt="png" /></p>

<p>Como pode ser visto a partir do gráfico acima, a maneira através da qual definimos a função $\theta$ é muito parecida com o que fizemos na <a href="/modelo-de-regressao-logistica/">parte 11</a>, quando estudamos o modelo de regressão logística. A razão de termos escolhido uma função com contradomínio ligeiramente diferente $-$ nesse caso, $\tanh$ $-$ é a facilidade que temos em lidar com sua derivada.</p>

<p>Agora, vamos estabelecer algumas notações que nos serão úteis ao longo do texto. As camadas (ou <em>labels</em>) serão denotadas por $l = 0, 1, 2, \cdots, L$; por exemplo, a camada $l = L$ é aquela à qual demos o nome de <em>output</em>. Para nos referirmos a uma determiada camada $l$, utilizaremos o sobrescrito ${}^{(l)}$. Cada <em>layer</em> tem dimensão $d^{(l)}$, o que significa que em $l$ existem $d^{(l)} + 1$ nós (lembre-se de $x_0$, chamado também de <em>bias</em>).</p>

<p>O conjunto de hipóteses para o modelo de rede neural será denotado por $\mathcal{H}_{nn}$, e é completamente especificado uma vez que é determinada a <em>arquitetura</em> da rede; ou seja, a dimensão <script type="math/tex">\mathbf{d} = [d^{(0)}, \cdots, d^{(L)}]</script> para todas as <em>layers</em>. Similarmente, uma hipótese <script type="math/tex">h \in \mathcal{H}_{nn}</script> é caracterizada pelo peso <script type="math/tex">w^{(l)}_{i j}</script> atribuído a cada “flecha” da rede. Vamos olhar de mais perto um par de nós da nossa rede.</p>

<p><img src="/assets/images/redes-neurais/uma-relacao.png" alt="Componentes entre um par de nós" />
<em>Figura 2 [fonte: “<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>”] $-$ Componentes entre um par de nós.</em></p>

<p>Da Fig. 2, um nó tem um sinal de entrada $s$ e um de saída $x$. O peso que conecta o nó $j$ (da camada $l$) ao nó $i$ (da camada <strong>anterior</strong>) é denotado por, como já havíamos visto, $w^{(l)}_{i j}$. Dessa forma, teremos $1 \leq l \leq L$, $0 \leq i \leq d^{(l - 1)}$ e $1 \leq j \leq d^{(l)}$.</p>

<p>Além disso, quando tivermos o interesse <em>macro</em> de analisar os termos da nossa rede; ou seja, olhar para as camadas como componentes únicas, podemos utilizar a notação vetorial. Para a coleção de sinais de entrada $1, \cdots, d^{(l)}$ na camada $l$, teremos $\mathbf{s}^{(l)}$. De forma parecida, para o conjunto de saídas em $l$, iremos utilizar o vetor $\mathbf{x}^{(l)}$ $-$ composto por elementos de índices $0, 1, \cdots, d^{(l)}$. Para as “flechas” que conectam $(l-1)$ à camada $l$, existirá uma matriz $(d^{(l-1)} + 1) \times d^{(l)}$ de pesos $W^{(l)}$, tal que a $(i, j)$-ésima entrada de $W^{(l)}$ é o termo $w^{(l)}_{ij}$. Por fim, todas as matrizes do tipo $W^{(l)}$ serão guardadas em um vetor $\mathbf{w} = [W^{(1)}, \cdots, W^{(L)}]$.</p>

<h3 id="forward-propagation">Forward Propagation</h3>

<p>Para calcularmos o valor de $h(\mathbf{x})$ (e, por consequência, o erro $e(h(\mathbf{x}), f(\mathbf{x})$), vamos utilizar o algoritmo “<em>Forward Propagation</em>” (ou, em português, algo como “Propagação para Frente”). Mas antes de qualquer outra coisa, observe que as entradas e saídas em uma camada $l$ podem ser relacionadas por:</p>

<script type="math/tex; mode=display">\begin{align*}
\mathbf{x}^{(l)} = \left[ 1, \theta(\mathbf{s}^{(l)})\right]^{\text{T}},
\end{align*}</script>

<p>onde <script type="math/tex">\theta(\mathbf{s}^{(l)})</script> é o vetor cujas componentes são <script type="math/tex">\theta(s^{(l)}_j)</script>. Dessa forma, temos que <script type="math/tex">s^{(l)}_j = \sum_{i = 0}^{d^{(l-1)}} w^{(l)}_{ij} x^{(l-1)}_i</script>. Em notação vetorial:</p>

<script type="math/tex; mode=display">\begin{align*}
\mathbf{s}^{(l)} = \left( W^{(l)} \right)^{\text{T}} \mathbf{x}^{(l-1)}.
\end{align*}</script>

<p>Note que, agora, só falta atribuírmos $\mathbf{x}$ a $\mathbf{x}_0$. Sendo assim, podemos utilizar o algoritmo abaixo para calcular $h(\mathbf{x})$.</p>

<p><img src="/assets/images/redes-neurais/algoritmo-forward.png" alt="Algoritmo Forward Propagation" />
<em>Figura 3 [fonte: “<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>”] $-$ Algoritmo Forward Propagation.</em></p>

<p>Calculado $x^{(L)} = h(\mathbf{x})$ podemos, finalmente, determinar $E_{in}$. Utilizando a medida de erro quadrático; isto é, $e(h(\mathbf{x}), f(\mathbf{x})) = (h(\mathbf{x}) - f(\mathbf{x}))^2$, temos:</p>

<script type="math/tex; mode=display">E_{in} = \frac{1}{N} \sum_{n = 1}^{N}(\mathbf{x}^{(L)}_n - f(\mathbf{x}_n))^2.</script>

<p>Agora, o que temos que fazer é, como de costume, minimizar $E_{in}$. Discutiremos isso na próxima seção.</p>

<h3 id="backpropagation-algorithm">Backpropagation Algorithm</h3>

<p>Na <a href="/modelo-de-regressao-logistica/">parte 11</a>, utilizamos o algoritmo <em>gradient descent</em> para encontrar $\mathbf{w}$ tal que $E_{in}(\mathbf{w})$ é mínimo (local). Bastou inicializar $\mathbf{w}(0)$ e, para $t = 1, 2, \cdots$, atualizar o vetor de pesos da seguinte maneira:</p>

<script type="math/tex; mode=display">\begin{align*}
\mathbf{w}(t+1) = \mathbf{w}(t) - \eta \nabla E_{in}(\mathbf{w}(t)).
\end{align*}</script>

<p>Porém, para implementar esse procedimento, precisamos encontrar o vetor gradiente. Poderíamos fazer isso “na mão”; mas é nesse ponto em que o <em>Backpropagation Algorithm</em> (ou “Algoritmo de Propagação para Trás”) entra em ação.</p>

<p>Esse algoritmo se baseia em aplicações sucessivas da “regra da cadeia” a fim de escrever as derivadas parciais na <em>layer</em> $l$ utilizando, para isso, as derivadas parciais de $l + 1$.</p>

<p>Comece definindo o “vetor sensitivo” ($\mathbf{\delta}^{(l)}$) para a camada $l$ da seguinte forma:</p>

<script type="math/tex; mode=display">\begin{align*}
\mathbf{\delta}^{(l)} = \frac{\partial e(\mathbf{w})}{\partial \mathbf{s}^{(l)}};
\end{align*}</script>

<p>ou seja, $\mathbf{\delta}^{(l)}$ é a derivada parcial da medida de erro $e(\mathbf{w})$ com respeito ao sinal de entrada $\mathbf{s}^{(l)}$. Dessa forma, podemos escrever a derivada de interesse como:</p>

<script type="math/tex; mode=display">\begin{align*}
\frac{\partial e(\mathbf{w})}{\partial W^{(l)}} = \mathbf{x}^{(l-1)}(\mathbf{\delta}^{(l)})^{\text{T}}
\end{align*}.</script>

<p>Aqui, note que $\frac{\partial \mathbf{s}^{(l)}}{\partial W^{(l)}} = \mathbf{x}^{(l-1)}$. Assim, como o termo $\mathbf{x}^{(l)}$, para $l \geq 0$, pode ser obtido por <em>forward propagation</em>, é suficiente que nos preocupemos apenas com $\mathbf{\delta}^{(l)}$. E é aqui que as coisas ficam interessantes. Com uma pequena modificação na rede e “rodando-a” <em>ao contrário</em> (por isso o nome “<em>backpropagation</em>”), conseguimos $\mathbf{\delta}^{(l)}$.</p>

<p>Ao invés de cada <em>layer</em> “cuspir” o vetor $\mathbf{x}^{(l)}$ (como acontece quando estamos no algoritmo <em>forward propagation</em>), fazendo o caminho inverso, cada camada irá devolver o vetor $\mathbf{\delta}^{(l)}$.</p>

<p>Em relação à “pequena modificação” que temos que fazer, agora cada nó faz uma transformação do tipo “multiplicação por $\theta^{\prime}(\mathbf{s}^{(l)})$”. Sendo assim, se $\theta(\cdot) = \tanh(\cdot)$, então $\tanh^{\prime}(\mathbf{s}^{(l)}) = 1-\tanh^2(\mathbf{s}^{(l)}) = 1 - \mathbf{x}^{(l)} \otimes\mathbf{x}^{(l)}$; onde $\otimes$ significa produto termo-a-termo. A imagem abaixo ilustra esse procedimento.</p>

<p><img src="/assets/images/redes-neurais/backpropagation.png" alt="Backpropagation" />
<em>Figura 4 [fonte: “<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>”] $-$ Esquema de “Backpropagation”.</em></p>

<p>Além disso, a partir da Fig. 4, vemos que a camada $(l+1)$ “cospe” (para trás) o vetor $\mathbf{\delta}^{(l+1)}$, que é multiplicado por $W^{(l+1)}$, somado, e entregue para os nós da <em>layer</em> $l$. Essa operação pode ser escrita como:</p>

<script type="math/tex; mode=display">\begin{align*}
\mathbf{\delta}^{(l)} = \theta^{\prime}(\mathbf{s}^{(l)}) \otimes \left[ W^{(l + 1)} \mathbf{\delta}^{(l+1)} \right]^{d^{(l)}}_1,
\end{align*}</script>

<p>onde o vetor $\left[ W^{(l + 1)} \mathbf{\delta}^{(l+1)} \right]^{d^{(l)}}_1$ contém as componentes de $W^{(l + 1)} \mathbf{\delta}^{(l+1)} $, excluíndo o valor de índice zero (excluindo o termo “<em>bias</em>”). <strong>Observação 1:</strong> a igualdade acima não é trivial; para ver o passo-a-passo de como obtê-la, consulte o capítulo e-7 de
<a href="">Learning from Data</a>.</p>

<p>Sendo assim, se temos $\mathbf{\delta}^{(l+1)}$, podemos encontrar $\mathbf{\delta}^{(l)}$. Para iniciar essa “cadeia”, basta determinar $\mathbf{\delta}^{(L)}$:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\mathbf{\delta}^{(L)} & = \frac{\partial e(\mathbf{w})}{\partial \mathbf{s}^{(L)}} \\
& = \frac{\partial}{\partial \mathbf{s}^{(L)}} (\mathbf{x}^{L} - f(\mathbf{x}))^2 \\
& 2(\mathbf{x}^{(L)} - f(\mathbf{x})) \frac{\partial \mathbf{x}^{(L)}}{\partial \mathbf{s}^{(L)}} \\
& 2(\mathbf{x}^{(L)} - f(\mathbf{x})) \theta^{\prime}(\mathbf{s}^{(L)}).
\end{align*} %]]></script>

<p><strong>Observação 2:</strong> quando a transformação de saída é $\theta(\cdot) = \tanh(\cdot)$, temos que $\theta^{\prime}(\mathbf{s}^{(L)}) = 1 - (\mathbf{x}^{(L)})^2$; porém, quando a transformação de saída é a função identidade, $\theta^{\prime}(\mathbf{s}^{(L)}) = 1$. <strong>Observação 3:</strong> se existir somente um nó de saída, $\mathbf{s}^{(L)}$ é escalar (e, por consequência, $\mathbf{\delta}^{(L)}$ também).</p>

<p>Por fim, utilizando a fórmula de $\mathbf{\delta}^{(l)}$, podemos calcular todos os “vetores sensitivos”. O algoritmo para esse tipo de dedução é apresentado a seguir (assumindo transformação nas <em>hidden layers</em> igual a $\tanh(\cdot)$ $-$ se esse não for o caso, adaptar o passo $3$).</p>

<p><img src="/assets/images/redes-neurais/calculate-sensitivity.png" alt="Algoritmo Backpropagation" />
<em>Figura 5 [fonte: “<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>”] $-$ Algoritmo “Backpropagation”.</em></p>

<p>Agora, para obter a derivada da medida de erro com respeito ao vetor de pesos, basta lembrar que $\frac{\partial e(\mathbf{w})}{\partial W^{(l)}} = \mathbf{x}^{(l-1)}(\mathbf{\delta}^{(l)})^{\text{T}}$.</p>

<p>Estamos quase lá! O que vamos fazer a seguir é calcular a derivada de $E_{in}(h)$, como gostaríamos que fosse feito (mas veremos na última parte desse texto que existe, nesse sentido, uma abordagem melhor: o <em>stochastic gradient descent</em>):</p>

<script type="math/tex; mode=display">\begin{align*}
\frac{\partial E_{in}}{\partial W^{(l)}} = \frac{1}{N} \sum^{N}_{n = 1} \frac{\partial e(\mathbf{x}_n)}{\partial W^{(l)}}.
\end{align*}</script>

<p>O algorimo a seguir, onde $G^{(l)}(\mathbf{x}_n)$ é o vetor gradiente no ponto $\mathbf{x}_n$, sumariza tudo que fizemos até agora.</p>

<p><img src="/assets/images/redes-neurais/algoritmo-gradiente.png" alt="Algoritmo Gradiente" />
<em>Figura 6 [fonte: “<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>”] $-$ Algoritmo para cálculo do vetor gradiente.</em></p>

<h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>

<p>Existe uma versão do algoritmo de gradiente descendente, conhecida como <em>Stochastic Gradient Descent</em>, que reduz consideravelmente o tempo computacional gasto para minimizar $E_{in}$.</p>

<p>No algoritmo de <em>gradient descent</em> que havíamos visto até então, o vetor gradiente era calculado para todo o conjunto de dados antes de sermos capazes de atualizar $\mathbf{w}$. Nessa nova versão, ao invés de considerar os $N$ pontos, escolha um $(\mathbf{x}_n, y_n)$ uniformemente em $\mathcal{D}$ e leve em conta apenas esse ‘par ordenado’ para calcular a derivada ($e^{\prime}(\mathbf{w})$); então, esse vetor gradiente obtido é utilizado para atualizar o vetor de pesos <strong>da mesma forma</strong> que fazíamos antes.</p>

<p>A ideia de o porquê isso funciona vem do fato de que:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\mathbb{E}_{\mathbf{x}_n}\left[\nabla e(h(\mathbf{x}_n), f(\mathbf{x}_n))\right] & = \frac{1}{N}\sum_{n = 1}^{N} \nabla e(h(\mathbf{x}_n), f(\mathbf{x}_n)) \\
& = \nabla E_{in}(h);
\end{align*} %]]></script>

<p>ou seja, “em média”, o vetor gradiente leva o processo de minimização para a direção correta (exceto por pequenos desvios).</p>

<p>Além de ser computacionalmente mais barato, o <em>Stochastic Gradient Descent</em> também nos ajuda a contornar problemas de “ficar preso em mínimos locais” $-$ já que a minimização, como dito, não é “direta”.</p>

<p>Se utilizarmos essa técnica para minimizar o termo $E_{in}$ na nossa rede neural, o algoritmo <strong>geral</strong> pode ser estabelecido como:</p>

<p><img src="/assets/images/redes-neurais/algoritmo-final.png" alt="Algoritmo Final" />
<em>Figura 7 [fonte: “<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>”] $-$ Algoritmo geral para rede neural.</em></p>

<h2 id="conclusão">Conclusão</h2>

<p>Ao longo do texto, vimos como resolver, pedaço por pedaço, os problemas que enfrentamos quando tentamos implementar um modelo do tipo “rede neural”. Vimos que, depois de construída a arquitetura da rede, calcular as medidas de erro não foi um problema $-$ bastou utilizar o algoritmo <em>Forward Propagation</em>. Porém, quando tentammos encontrar $\mathbf{w}$ que minimiza $E_{in}$, percebemos que definir o vetor gradiente $\nabla E_{in}(\mathbf{w})$ não é tarefa fácil. Aqui, o <em>Backpropagation Algorithm</em> nos foi muito útil e resolveu esse problema. Por fim, vimos a vantagem de implementar o algoritmo <em>Stochastic Grandient Descent</em> ao invés do gradiente descendente “normal” que havíamos visto no post passado. Na próxima postagem, vamos fazer a implementação em Python do que estudamos hoje.</p>

<p>Qualquer dúvida, sugestão ou <em>feedback</em>, por favor, deixe um comentário abaixo.</p>

<blockquote>
  <p>Essa postagem faz parte de uma <a href="/categorias/#machine-learning-learning-from-data">série</a> de textos que tem o objetivo de estudar, principalmente, o curso “<a href="http://www.work.caltech.edu/telecourse.html">Learning from Data</a>”, ministrado pelo Professor Yaser Abu-Mostafa. Outros materiais utilizados serão sempre referenciados.</p>
</blockquote>

        
      </section>

      <footer class="page__meta">
        
        


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Atualizado em:</strong> <time datetime="2020-01-28T00:00:00-03:00">January 28, 2020</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/modelo-de-regressao-logistica/" class="pagination--pager" title="[Parte 11] Modelo de Regressão Logística &amp; Implementação em Python
">Anterior</a>
    
    
      <a href="/implementacao-redes-neurais/" class="pagination--pager" title="[Parte 13] Implementação em Python: Redes Neurais
">Próxima</a>
    
  </nav>

    </div>

    
      <div class="page__comments">
  
  
      <h4 class="page__comments-title">Deixe um comentário</h4>
      <section id="disqus_thread"></section>
    
</div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">Talvez você também goste</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/implementacao-redes-neurais/" rel="permalink">[Parte 13] Implementação em Python: Redes Neurais
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  16 minuto(s) de leitura

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Como continuação direta da parte 12 dessa série de textos, vamos, ao longo desse post, implementar uma rede neural utilizando Python. E, para testá-la, vamos resolver um problema de classificação binária de dados que não são linearmente separáveis $-$ aqui, não faremos como na parte 06, onde utilizamos transformaçõe...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/modelo-de-regressao-logistica/" rel="permalink">[Parte 11] Modelo de Regressão Logística &amp; Implementação em Python
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  15 minuto(s) de leitura

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Voltando a falar de modelos lineares, como os de classificação $-$ explorados nas partes 02 e 04 $-$ ou de regressão linear, a exemplo do que vimos na parte 05, iremos discutir nesse post o que é e como funciona a regressão logística. Veremos que esse novo modelo herda características das duas classes de algoritmos ...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/vies-variancia-tradeoff/" rel="permalink">[Parte 10] Trade-off entre Viés-Variância
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  7 minuto(s) de leitura

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Continuando com a discussão, introduzida na parte 09 dessa série de textos, de que deve existir um “meio termo” sobre o tamanho de $\mathcal{H}$ $-$ lembre-se: se $\mathcal{H}$ é muito grande, conseguimos diminuir o termo $E_{in}$; porém, somos penalizados na generalização do modelo para dados fora de $\mathcal{D}$....</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/dimensao-vc/" rel="permalink">[Parte 09] Dimensão de Vapnik-Chervonenkis
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  9 minuto(s) de leitura

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Continuando a discussão que começamos na última parte da nossa série de textos, vamos estudar mais propriedades associadas ao que demos o nome de “Teoria da Generalização”; ou, em outras palavras, ao estudo de como os nossos modelos podem ser generalizados para conjuntos de dados fora de $\mathcal{D}$.
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Informações:</strong></li>
    

    
      
        
          <li><a href="https://github.com/avramaral" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://twitter.com/avramaral" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/andre-victor-ribeiro-amaral/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin-in" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://www.youtube.com/channel/UC5RVscKF68DWYixtJPrBccQ" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-youtube" aria-hidden="true"></i> YouTube</a></li>
        
      
    

    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 André. Desenvolvido com <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>







    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/redes-neurais/";  // Replace PAGE_URL with your page's canonical URL variable
      this.page.identifier = "/redes-neurais"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://blog-do-andre.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  





    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  </body>
</html>
