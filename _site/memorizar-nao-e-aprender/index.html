<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.17.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="pt" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>[Parte 03] Memorizar não é Aprender - Blog do André</title>
<meta name="description" content="O que nós fizemos até agora foi, dado um conjunto $\mathcal{D}$, treinar um modelo que “explicasse” (no caso do Perceptron, “classificasse”) os dados $(\mathbf{x}_1, y_1), \cdots, (\mathbf{x}_N, y_N)$. Porém, isso significa, de fato, aprender? Isto é, a função $g \in \mathcal{H}$ escolhida pelo algoritmo aproxima a função alvo $f$ no sentido de ter bom desempenho em explicar $\mathbf{x} \not\in \mathcal{D}$?">


  <meta name="author" content="André V. R. Amaral">


<meta property="og:type" content="article">
<meta property="og:locale" content="pt_BR">
<meta property="og:site_name" content="Blog do André">
<meta property="og:title" content="[Parte 03] Memorizar não é Aprender">
<meta property="og:url" content="http://localhost:4000/memorizar-nao-e-aprender/">


  <meta property="og:description" content="O que nós fizemos até agora foi, dado um conjunto $\mathcal{D}$, treinar um modelo que “explicasse” (no caso do Perceptron, “classificasse”) os dados $(\mathbf{x}_1, y_1), \cdots, (\mathbf{x}_N, y_N)$. Porém, isso significa, de fato, aprender? Isto é, a função $g \in \mathcal{H}$ escolhida pelo algoritmo aproxima a função alvo $f$ no sentido de ter bom desempenho em explicar $\mathbf{x} \not\in \mathcal{D}$?">







  <meta property="article:published_time" content="2020-01-08T00:00:00-03:00">





  

  


<link rel="canonical" href="http://localhost:4000/memorizar-nao-e-aprender/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "André",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Blog do André Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<link rel="shortcut icon" href="../../assets/images/favicon.ico" type="image/x-icon">

<!-- end custom head snippets -->


    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
    </script>
    
  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Blog do André
          <span class="site-subtitle">Probabilidade, estatística e mais.</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/categorias/" >Categorias</a>
            </li><li class="masthead__menu-item">
              <a href="/busca/" >Busca</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/profile_picture.jpeg" alt="André V. R. Amaral" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">André V. R. Amaral</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Mestrando em Estatística pela <a href="http://www.est.ufmg.br/portal/">Universidade Federal de Minas Gerais</a>.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Informações</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="/sobre/" rel="nofollow noopener noreferrer"><i class="far fa-fw fa-question-circle" aria-hidden="true"></i> Sobre</a></li>
          
        
          
            <li><a href="mailto:avramaral@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> E-mail</a></li>
          
        
          
            <li><a href="https://github.com/avramaral" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          
        
          
            <li><a href="https://twitter.com/avramaral" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i> Twitter</a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/andre-victor-ribeiro-amaral/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin-in" aria-hidden="true"></i> LinkedIn</a></li>
          
        
          
            <li><a href="https://www.youtube.com/channel/UC5RVscKF68DWYixtJPrBccQ" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-youtube" aria-hidden="true"></i> YouTube</a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="[Parte 03] Memorizar não é Aprender">
    <meta itemprop="description" content="O que nós fizemos até agora foi, dado um conjunto $\mathcal{D}$, treinar um modelo que “explicasse” (no caso do Perceptron, “classificasse”) os dados $(\mathbf{x}_1, y_1), \cdots, (\mathbf{x}_N, y_N)$. Porém, isso significa, de fato, aprender? Isto é, a função $g \in \mathcal{H}$ escolhida pelo algoritmo aproxima a função alvo $f$ no sentido de ter bom desempenho em explicar $\mathbf{x} \not\in \mathcal{D}$?">
    <meta itemprop="datePublished" content="January 08, 2020">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">[Parte 03] Memorizar não é Aprender
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  5 minuto(s) de leitura

</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>O que nós fizemos até agora foi, dado um conjunto $\mathcal{D}$, treinar um modelo que “explicasse” (no caso do Perceptron, “classificasse”) os dados $(\mathbf{x}_1, y_1), \cdots, (\mathbf{x}_N, y_N)$. Porém, isso significa, de fato, aprender? Isto é, a função $g \in \mathcal{H}$ escolhida pelo algoritmo aproxima a função alvo $f$ no sentido de ter bom desempenho em explicar $\mathbf{x} \not\in \mathcal{D}$?</p>

<p>O que nós vamos fazer agora é introduzir uma componente aleatória no <em>framework</em> de aprendizado que começamos a discutir na <a href="/o-que-e-aprendizado/">parte 01</a> <a href="/categorias/#machine-learning-learning-from-data">dessa série de textos</a> $-$ que vai nos permitir responder “sim” às perguntas do parágrafo anterior.</p>

<p>Para <script type="math/tex">h \in \mathcal{H}</script>, seja <script type="math/tex">\mu = \mathbb{P}\left[h(\mathbf{x}_n) \neq f(\mathbf{x}_n)\right]</script>, com <script type="math/tex">n = 1, \cdots, N</script>; i.e., a probabilidade de que o valor de uma função fixa <script type="math/tex">h</script>  avaliada em <script type="math/tex">\mathbf{x}_n</script> <script type="math/tex">-</script> com <script type="math/tex">h</script> escolhida antes de <script type="math/tex">\mathcal{D}</script> ser gerado <script type="math/tex">-</script> seja diferente da função alvo $f$ avaliada no mesmo ponto. Por consequência, <script type="math/tex">1 - \mu = \mathbb{P}\left[h(\mathbf{x}_n) = f(\mathbf{x}_n)\right]</script>. Além disso, defina <script type="math/tex">\nu = \frac{1}{N} \sum_{n= 1}^{N} \mathbb{I}_{\lbrace h(\mathbf{x}_n) \neq f(\mathbf{x}_n)\rbrace}</script>; ou seja, <script type="math/tex">\nu</script> é a proporção de vezes que <script type="math/tex">h(\mathbf{x}_n)</script> é diferente de <script type="math/tex">f(\mathbf{x}_n)</script> para uma amostra <script type="math/tex">\mathcal{D}</script>. A ideia é que, desde que a <script type="math/tex">\mathcal{D}</script> seja gerada aleatoriamente seguindo uma distribuição <script type="math/tex">P</script> (não necessariamente conhecida), então <script type="math/tex">\nu</script> aproxima bem <script type="math/tex">\mu</script>. A relação a seguir, conhecida como <em>Desigualdade de Hoeffding</em>, quantifica essa aproximação:</p>

<script type="math/tex; mode=display">\begin{align*}
    \mathbb{P}\left[\lvert \nu - \mu\rvert > \epsilon\right] \leq 2e^{-2 \epsilon^2 N} \text{, para todo N e }\forall \epsilon > 0.
\end{align*}</script>

<p>O que a inequação acima diz é que a probabilidade de $\nu$ estar arbitrariamente próximo de $\mu$ é algo como “$1$ menos alguma coisa que decai exponencialmente com $N$”. O que é o mesmo que dizer que, para $N$ “grande”, $\nu$ aproxima bem o comportamento de $\mu$. Assim, a quantidade de vezes que $h$ erra na amostra $\mathcal{D}$ é proporcional à quantidade de erros que $h$ cometeria fora de $\mathcal{D}$. Veja abaixo um esquema atualizado das nossas componentes do aprendizado.</p>

<p><img src="/assets/images/memorizar-nao-e-aprender_files/comp-aprendiz-estocastico.png" alt="Framework de aprendizado com componente estocástica" />
<em>Figura 1 [fonte: “<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>”] $-$ Framework de aprendizado atualizado com componente estocástica.</em></p>

<p>Dito tudo isso, se houvesse apenas uma função em $\mathcal{H}$, seria fácil de verificar se $h$ tem bom desempenho em avaliar pontos fora de $\mathcal{D}$; só que esse não é o caso $-$ na maior parte das vezes, $\mathcal{H}$ tem cardinalidade infinita, inclusive. Isso nos motiva a introduzir uma nova notação:</p>

<p>Defina <script type="math/tex">E_{in}(h) = \frac{1}{N} \sum_{n= 1}^{N}\mathbb{I}_{\lbrace h(\mathbf{x}_n) \neq f(\mathbf{x}_n)\rbrace}</script> e <script type="math/tex">E_{out}(h) = \mathbb{P}\left[h(\mathbf{x}_n) \neq f(\mathbf{x}_n)\right]</script>; ou seja, <script type="math/tex">E_{in}(h)</script> e <script type="math/tex">E_{out}(h)</script> são, respectivamente, as quantidades <script type="math/tex">\nu</script> e <script type="math/tex">\mu</script> <strong>como função de <script type="math/tex">h</script></strong>. Então é óbvio que, para todo <script type="math/tex">N</script>, <script type="math/tex">\mathbb{P}\left[\lvert E_{in}(h) - E_{out}(h)\rvert > \epsilon\right] \leq 2e^{-2 \epsilon^2 N}</script>, <script type="math/tex">\forall \epsilon > 0</script>. O ganho em definir essa nova notação aparece no próximo parágrafo. <strong>Observação:</strong> o subscrito “<script type="math/tex">in</script>” faz referência ao termo <em>in-sample</em>; da mesma forma, “<script type="math/tex">out</script>” quer dizer <em>out-of-sample</em>.</p>

<p>A cota que temos até agora diz respeito a uma única função $h \in \mathcal{H}$ ; porém, se $\mathcal{H}$ tem cardinalidade maior que $1$ (o que, na prática, é sempre verdade), podemos escrever uma relação parecida para uma função $g \in \mathcal{H}$ escolhida por $\mathcal{A}$. Seja $\mathcal{H}$ conjunto finito de tamanho $M$, então vale:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\mathbb{P}\left[\lvert E_{in}(g) - E_{out}(g)\rvert > \epsilon\right] & \leq \mathbb{P}\left[\bigcup_{m = 1}^{M} \left[ \lvert E_{in}(h_m) - E_{out}(h_m)\rvert > \epsilon \right] \right] \\
& \leq \sum_{m = 1}^{M} \mathbb{P}\left[\lvert E_{in}(h_m) - E_{out}(h_m)\rvert > \epsilon \right] \\
& \leq \sum_{m = 1}^{M} 2e^{-2 \epsilon^2 N} = 2 M e^{-2 \epsilon^2 N}.
\end{align*} %]]></script>

<p>Nas equivalências acima, a primeira desigualdade é justificada por inclusão de eventos, a segunda por <em>union bound</em>, e a terceira, como já vimos, vem da Desigualdade de Hoeffding.</p>

<p>A princípio, essa cota que conseguimos para $g$ só faz sentido se $M$ for finito; já que o lado direito da desigualdade cresce com $M$. Entretanto, esse resultado pode ser generalizado para $\mathcal{H}$ conjunto infinito.</p>

<p>Em resumo, é possível interpretar o resultado de que $\mathbb{P}\left[\lvert E_{in}(g) - E_{out}(g)\rvert &gt; \epsilon\right] \leq 2 M e^{-2 \epsilon^2 N}$ da seguinte forma: a depender de $M$ e $\epsilon$, o nosso modelo consegue, de fato, <strong>aprender</strong>, pois a função $g \in \mathcal{H}$ escolhida por $\mathcal{A}$ se aproxima de $f$ quando $N$ cresce $-$ no sentido de, se $E_{in}(g) \approx 0$, ter probabilidade de erro arbitrariamente pequena para avaliar $\mathbf{x} \not\in \mathcal{D}$.</p>

<h2 id="conclusão">Conclusão</h2>

<p>Vimos que, com a introdução de uma componente estocástica no nosso <em>framework</em> de aprendizado, é possível que nosso modelo aprenda (e não apenas memorize). Nesse caso, quando $N$ cresce, $E_{in}(g) \approx E_{out}(g)$. Assim, se conseguirmos fazer com que $E_{in}(g) \approx 0$, então teremos que $E_{out}(g) \approx 0$; o que é o mesmo que dizer que se o algoritmo $\mathcal{A}$ conseguir escolher uma função $g \in \mathcal{H}$ que tem bom desempenho em avaliar $\mathbf{x} \in \mathcal{D}$, então $g$ também terá bons resultados para observações fora de $\mathcal{D}$.</p>

<p>Qualquer dúvida, sugestão ou <em>feedback</em>, por favor, deixe um comentário abaixo.</p>

<blockquote>
  <p>Essa postagem faz parte de uma <a href="/categorias/#machine-learning-learning-from-data">série</a> de textos que tem o objetivo de estudar, principalmente, o curso “<a href="http://www.work.caltech.edu/telecourse.html">Learning from Data</a>”, ministrado pelo Professor Yaser Abu-Mostafa. Outros materiais utilizados serão sempre referenciados.</p>
</blockquote>

        
      </section>

      <footer class="page__meta">
        
        


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Atualizado em:</strong> <time datetime="2020-01-08T00:00:00-03:00">January 08, 2020</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/implementacao-perceptron/" class="pagination--pager" title="[Parte 02] Implementação em Python: Perceptron
">Anterior</a>
    
    
      <a href="/modelos-lineares-de-classificacao-e-pocket/" class="pagination--pager" title="[Parte 04] Modelos Lineares de Classificação &amp; Implementação em Python do Pocket
">Próxima</a>
    
  </nav>

    </div>

    
      <div class="page__comments">
  
  
      <h4 class="page__comments-title">Deixe um comentário</h4>
      <section id="disqus_thread"></section>
    
</div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">Talvez você também goste</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/modelo-de-regressao-linear/" rel="permalink">[Parte 05] Modelo de Regressão Linear &amp; Implementação em Python
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  12 minuto(s) de leitura

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Continuando a discussão sobre modelos lineares, assunto que começamos a estudar na parte 04 dessa série de textos, vamos, nesse post, falar do modelo de regressão. A principal diferença entre esse tipo de modelo e os classificadores que estudamos anteriormente é que agora a função alvo $f$ é real-avaliada $-$ que é ...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/modelos-lineares-de-classificacao-e-pocket/" rel="permalink">[Parte 04] Modelos Lineares de Classificação &amp; Implementação em Python do Pocket
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  11 minuto(s) de leitura

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Na parte 03 dessa série de textos, discutimos se é possível que nossos algoritmos, de fato, aprendam; ou seja, se conseguimos determinar uma função $g \in \mathcal{H}$ que aproxima bem $f$ para pontos $\mathbf{x} \not\in \mathcal{D}$. Esse assunto ainda não foi esgotado, mas por hora vamos nos concentrar em estudar ...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/implementacao-perceptron/" rel="permalink">[Parte 02] Implementação em Python: Perceptron
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  10 minuto(s) de leitura

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Como mencionado na parte 01 dessa série de textos, essa postagem será dedicada à implementação do Perceptron Learning Algorithm (PLA) em Python, utilizando, para isso, a biblioteca Numpy. Discutiremos um exemplo e, ao final, vamos ver como utilizar a implementação desse mesmo algoritmo pela biblioteca Sklearn.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/o-que-e-aprendizado/" rel="permalink">[Parte 01] O que é Aprendizado (de Máquina) &amp; PLA
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  10 minuto(s) de leitura

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Quando se fala de Aprendizado de Máquina, há pelo menos duas interpretações para o que isso significa: a primeira delas diz respeito a um conjunto de técnicas e modelos estatísticos que são usados, primariamente, para se fazer inferência (estimação pontual dos parâmetros de interesse, contrução de intervalo de confi...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Informações:</strong></li>
    

    
      
        
          <li><a href="https://github.com/avramaral" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://twitter.com/avramaral" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/andre-victor-ribeiro-amaral/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin-in" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://www.youtube.com/channel/UC5RVscKF68DWYixtJPrBccQ" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-youtube" aria-hidden="true"></i> YouTube</a></li>
        
      
    

    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 André. Desenvolvido com <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>







    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/memorizar-nao-e-aprender/";  // Replace PAGE_URL with your page's canonical URL variable
      this.page.identifier = "/memorizar-nao-e-aprender"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://blog-do-andre.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  





    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  </body>
</html>
