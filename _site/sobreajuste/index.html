<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.17.2 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="pt" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>[Parte 14] Sobreajuste (ou Overfitting) - Blog do André</title>
<meta name="description" content="Continuando com a nossa série de textos, e agora falando sobre um assunto um pouco diferente do que vínhamos discutindo até então; vamos tratar do problema de sobreajuste (ou overfitting, do inglês $-$ como é mais conhecido). Veremos como a ideia de “ruído” (apresentada na parte 07) é causa direta desse tipo de fenômento e vamos, ainda, introduzir um novo conceito, o de “ruído determinístico”.">


  <meta name="author" content="André V. R. Amaral">


<meta property="og:type" content="article">
<meta property="og:locale" content="pt_BR">
<meta property="og:site_name" content="Blog do André">
<meta property="og:title" content="[Parte 14] Sobreajuste (ou Overfitting)">
<meta property="og:url" content="http://localhost:4000/sobreajuste/">


  <meta property="og:description" content="Continuando com a nossa série de textos, e agora falando sobre um assunto um pouco diferente do que vínhamos discutindo até então; vamos tratar do problema de sobreajuste (ou overfitting, do inglês $-$ como é mais conhecido). Veremos como a ideia de “ruído” (apresentada na parte 07) é causa direta desse tipo de fenômento e vamos, ainda, introduzir um novo conceito, o de “ruído determinístico”.">







  <meta property="article:published_time" content="2020-02-06T00:00:00-03:00">





  

  


<link rel="canonical" href="http://localhost:4000/sobreajuste/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "André",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Blog do André Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<link rel="shortcut icon" href="../../assets/images/favicon.ico" type="image/x-icon">

<!-- end custom head snippets -->


    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
    </script>
    
  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Blog do André
          <span class="site-subtitle">Probabilidade, estatística e mais.</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/categorias/" >Categorias</a>
            </li><li class="masthead__menu-item">
              <a href="/busca/" >Busca</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/profile_picture.jpeg" alt="André V. R. Amaral" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">André V. R. Amaral</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Mestrando em Estatística pela <a href="http://www.est.ufmg.br/portal/">Universidade Federal de Minas Gerais</a>.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Informações</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="/sobre/" rel="nofollow noopener noreferrer"><i class="far fa-fw fa-question-circle" aria-hidden="true"></i> Sobre</a></li>
          
        
          
            <li><a href="mailto:avramaral@gmail.com" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> E-mail</a></li>
          
        
          
            <li><a href="https://github.com/avramaral" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
          
        
          
            <li><a href="https://twitter.com/avramaral" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i> Twitter</a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/andre-victor-ribeiro-amaral/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin-in" aria-hidden="true"></i> LinkedIn</a></li>
          
        
          
            <li><a href="https://www.youtube.com/channel/UC5RVscKF68DWYixtJPrBccQ" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-youtube" aria-hidden="true"></i> YouTube</a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="[Parte 14] Sobreajuste (ou Overfitting)">
    <meta itemprop="description" content="Continuando com a nossa série de textos, e agora falando sobre um assunto um pouco diferente do que vínhamos discutindo até então; vamos tratar do problema de sobreajuste (ou overfitting, do inglês $-$ como é mais conhecido). Veremos como a ideia de “ruído” (apresentada na parte 07) é causa direta desse tipo de fenômento e vamos, ainda, introduzir um novo conceito, o de “ruído determinístico”.">
    <meta itemprop="datePublished" content="February 06, 2020">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">[Parte 14] Sobreajuste (ou Overfitting)
</h1>
          
            <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  11 minuto(s) de leitura

</p>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>Continuando com a nossa <a href="/categorias/#machine-learning-learning-from-data">série de textos</a>, e agora falando sobre um assunto um pouco diferente do que vínhamos discutindo até então; vamos tratar do problema de <strong>sobreajuste</strong> (ou <em>overfitting</em>, do inglês $-$ como é mais conhecido). Veremos como a ideia de “ruído” (apresentada na <a href="/erro-e-ruido/">parte 07</a>) é causa direta desse tipo de fenômento e vamos, ainda, introduzir um novo conceito, o de “ruído determinístico”.</p>

<p><strong><em>Overfitting</em></strong> pode ser entendido como o fenômeno no qual um bom ajuste do modelo escolhido com dados pertencentes ao conjunto de trainamento NÃO se traduz em $E_{out}$ proporcionalmente pequeno; na verdade, o contrário pode acontecer: o erro fora da amostra pode aumentar conforme o erro dentro da amostra diminui.</p>

<p>Para conseguirmos enxergar o que definimos como <em>overfitting</em> acontecendo, considere o exercício a seguir.</p>

<p><strong>Exemplo:</strong> trabalhando com conjunto de dados em $1$ dimensão e ajustando modelos da classe de regressão polinomial (esse é só um nome diferente para uma regressão linear com uma transformação do tipo $x \mapsto (1, x, x^2, \cdots)$ para dados unidimensionais), vamos definir dois cenários:</p>

<ol>
  <li>A função alvo é um polinômio de ordem $10$ <strong>com</strong> ruído associado. Aqui, $\mathcal{D}$ contém 15 pontos.</li>
  <li>A função alvo é um polinômio de ordem $50$ <strong>sem</strong> ruído associado. Aqui, $\mathcal{D}$ também contém 15 pontos.</li>
</ol>

<p>A figura a seguir ilustra o que acabamos de descrever:</p>

<p><img src="/assets/images/sobreajuste_files/exemplo_inicial.png" alt="Ilustração do exemplo" />
<em>Figura 1 [fonte: “<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>”] $-$ Ilustração do exemplo.</em></p>

<p>Agora suponha que, para lidar com essas funções alvo, vamos ajustar dois modelos de regressão: um de ordem $2$ e outro de ordem $10$.</p>

<p><img src="/assets/images/sobreajuste_files/dados_ajustados.png" alt="Modelos ajustados" />
<em>Figura 2 [fonte: “<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>”] $-$ Modelos ajustados.</em></p>

<p>Em seguida, vamos ver como os erros ($E_{in}$ e $E_{out}$) se comportam para esses dois modelos ajustados. <strong>Observação:</strong> lembre-se de que, como ainda estamos trabalhando com modelos de regressão, a medida de erro mais utilizada é o “erro quadrático”, como vimos pela primeira vez na <a href="/modelo-de-regressao-linear/">parte 05</a>.</p>

<p><img src="/assets/images/sobreajuste_files/tabela_erros.png" alt="Tabela de erros" />
<em>Figura 3 [fonte: “<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>”] $-$ Tabela de erros para todos os cenários considerados.</em></p>

<p>Vamos, primeiro, analisar a situação na qual a função alvo é um polinômio de grau $10$ <strong>com</strong> ruído. Nesse caso, note que o modelo mais simples (polinômio de grau $2$) teve erro fora da amostra (<script type="math/tex">E_{out}</script>) menor $-$ se comparado ao modelo ajustado com polinômio de grau $10$. Isso aconteceu porque, apesar de o conjunto de hipóteses <script type="math/tex">\mathcal{H}_{10}</script> conter a função alvo (<strong>a menos do ruído</strong>), a quantidade de dados de treinamento não foi suficiente para permitir generalização; sendo assim, o modelo apenas “memorizou” $\mathcal{D}$ e, por isso, teve $E_{in}$ muito pequeno. Dizemos que, aqui, houve <strong>sobreajuste</strong>. Observe, por fim, que, contrário à ideia de que “mais informações” se traduz em um modelo melhor, vimos que um modelo mais simples apresentou erro fora da amostra (que, no final das contas, é o que importa) bem mais interessante.</p>

<p>Agora, vamos olhar para a situação onde a função alvo é um polinômio de grau $50$ <strong>sem</strong> ruído. Aqui, mais uma vez, o modelo mais complexo $-$ o polinômio de grau $10$ - teve erro dentro da amostra menor; porém, perdeu “muito feio” para o desempenho do modelo mais simples para $x \not\in \mathcal{D}$. Nesse caso, também houve <strong>sobreajuste</strong>. Entretanto, a razão aqui é outra: no primeiro cenário, onde existia ruído associado a $f$, o que aconteceu foi que o modelo com mais parâmetros incorporou $\epsilon$ como parte do que seria a função alvo (uma amostra muito maior preveniria esse comportamento); agora, nesse segundo cenário, não há ruído. Sendo assim, o que aconteceu? A resposta é que $f$ é muito mais complexa que os dois possíveis conjuntos de hipóteses ($\mathcal{H_2}$ e <script type="math/tex">\mathcal{H}_{10}</script>); dessa forma, o algoritmo $\mathcal{A}$ tenta usar $\mathcal{H}_{10}$ para  modelar uma função que ele não é capaz, e, por isso, acaba “memorizando” os dados ao invés de, de novo, “aprender”.</p>

<p>A ideia aqui é que o <em>overfitting</em> pode estar relacionado a, principalmente, duas coisas: o nível de ruído (que denotaremos por $\sigma^2$) e a complexidade da função alvo (que denotaremos por $Q_f$). Ao primeiro distúrbio, danos o nome de <strong><em>ruído estocástico</em></strong> (<strong>não há nada de novo aqui</strong>, estamos apenas utilizando um termo maior para falar do mesmo “ruído” que temos considerado até então); ao passo que, ao segundo, damos o nome de <strong><em>ruído determinístico</em></strong>. Em ambos os casos, $g \in \mathcal{H}$ perde o poder de generalização para dados fora da amostra.</p>

<p>Dito isso, podemos tentar estebeler uma <strong>medida de sobreajuste</strong>. Nesse sentido, defina:</p>

<script type="math/tex; mode=display">\begin{align}
\text{Medida de sobreajuste } (\mathcal{M}_s) = E_{out}(g_{10}) - E_{out}(g_{2}),
\end{align}</script>

<p>onde $g_{10}$ e $g_{2}$ podem ser substituídas pelas funções (do tipo: $g \in \mathcal{H}$) escolhidas pelo modelo mais complexo e pelo modelo mais simples, respectivamente.</p>

<p>Assim, se $\mathcal{M}_s$ for positivo, quer dizer que o modelo mais simples ganha (que, em outras palavras, é o mesmo que dizer que o modelo complexo generaliza mal dados fora de $\mathcal{D}$); e o inverso acontece no caso de $\mathcal{M}_s$ ser negativo. Perceba que essa medida é fundamentalmente de comparação.</p>

<p>A imagem a seguir apresenta o resultado de um processo de simulação que estuda essas quantidades. A explicação do procedimento vem imediatamente abaixo.</p>

<p><img src="/assets/images/sobreajuste_files/heat_map.png" alt="Simulação para a quantidade &quot;medida de sobreajuste&quot;" />
<em>Figura 4 [fonte: “<a href="http://www.work.caltech.edu/textbook.html">Learning from Data</a>”] $-$ Simulação para a quantidade “$E_{out}(g_{10}) - E_{out}(g_{2})$”.</em></p>

<p>No mapa de calor da esquerda, vemos como a medida de sobreajuste (definida pela cor) depende de $\sigma^2$ e do tamanho da amostra (com $Q_f = 20$ fixo). Perceba que, nesse caso, se o ruído estocástico aumenta e o $N$ é pequeno, modelos mais complexos (nesse caso, $\mathcal{H}_{10}$) tem desempenho ruim; entretanto, se o tamanho da amostra aumenta, esse efeito é corrigido.</p>

<p>Já no mapa de calor da direita, é possível enxergar como a medida de sobreajuste depende da complexidade da função alvo e, mais uma vez, do tamanho da amostra (com $\sigma^2 = 0.1$ fixo). Note que, quando $Q_f &gt; 10$, a classe de modelos polinomiais de ordem $10$ começa a perder capacidade de aprendizado, abrindo espaço para que o modelo mais simples tenha melhor desempenho no que se diz respeito a $E_{out}$. Isso, de novo, é corrigido com o aumento do tamanho da amostra.</p>

<p>Aqui, é possível perceber o porquê do “ruído determinístico” receber esse nome. Quando a classe de modelos passa a não ser capaz mais de representar adequadamente a função alvo, a porção de dados não explicada pelo modelo é tratada como uma espécie de ruído.</p>

<h3 id="overfitting-e-o-trade-off-entre-viés-e-variância"><em>Overfitting</em> e o <em>trade-off</em> entre viés e variância</h3>

<p>Antes de finalizarmos, vamos ver como a questão do “<em>trade-off</em> entre viés e variância” se relaciona com as quantidades que acabamos de estudar. Relembrando o que foi apresentado na <a href="/vies-variancia-tradeoff/">parte 10</a>, podemos decompor o valor esperado do erro fora da amostra em duas componentes: viés e variância. A equação a seguir representa essa relação:</p>

<script type="math/tex; mode=display">\begin{align*}
\mathbb{E}_{\mathcal{D}}\left[E_{out}(g^{(\mathcal{D})})\right] = \text{Var} + \text{Viés},
\end{align*}</script>

<p>onde <script type="math/tex">\text{Viés} = \mathbb{E}_{\mathbf{x}}(\bar{g}(\mathbf{x}) - f(\mathbf{x}))^2</script> e que <script type="math/tex">\text{Var} = \mathbb{E}_{\mathcal{D},\mathbf{x}}\left[( g^{(\mathcal{D})}(\mathbf{x}) - \bar{g}(\mathbf{x}))^2\right]</script>.</p>

<p>Lembre-se, porém, de que na equação acima, <strong>não</strong> consideramos que existia ruído. Alternativamente, se agora dissermos que $y = f(x) + \epsilon$ (tal que $\mathbb{E}(\epsilon) = 0$ e $\mathbb{V}(\epsilon) = \sigma^2$), então, de maneira análoga, podemos deduzir que</p>

<script type="math/tex; mode=display">\begin{align*}
\mathbb{E}_{\mathcal{D}, \epsilon}\left[E_{out}(g^{(\mathcal{D})})\right] = \text{Var} + \text{Viés} + \sigma^2,
\end{align*}</script>

<p>com <script type="math/tex">\sigma^2 = \mathbf{E}_{\epsilon,\mathbf{x}}\left[(\epsilon(\mathbf{x}))^2\right]</script>.</p>

<p>Da equação acima, perceba que o <script type="math/tex">\text{Viés} = \mathbb{E}_{\mathbf{x}}(\bar{g}(\mathbf{x}) - f(\mathbf{x}))^2</script> pode ser visto como o que chamamos de “ruído determinístico” $-$ à medida que essa quantidade captura a inabilidade do modelo de aproximar a função alvo $f$. Aqui, lembre-se de que <script type="math/tex">\bar{g}(\mathbf{x}) = \mathbb{E}_{\mathcal{D}}\left[g^{(\mathcal{D})}(\mathbf{x})\right]</script>.</p>

<p>No final das contas, o que toda essa última seção quer dizer é que, similarmente ao que já fizemos antes, o valor esperado para o erro fora da amostra pode ser decomposto em:</p>

<ul>
  <li>“Ruído determinístico” e “ruído aleatório” que, dado um conjunto de hipóteses $\mathcal{H}$, são quantidades fixas; e</li>
  <li>”$\text{Var}$”, que é afetada indiretamente pelos dois tipos de ruídos - no sentido de que o modelo torna-se mais suscetível às variações advindas das componentes de ruído.</li>
</ul>

<h2 id="conclusão">Conclusão</h2>

<p>Nesse post, introduzimos a ideia de <strong>sobreajuste</strong>, bem como quais quantidades estão relacionadas a esse fenômeno: o “ruído estocástico” e o “ruído determinístico” (que tem ligação direta com a complexidade da função alvo). Via de regra, vimos que: ${}^{1)}$ se o tamanho de $N$ cresce, então $\mathcal{M}_s$ decresce, ${}^{2)}$ se o ruído estocástico aumenta, $\mathcal{M}_s$ também assume valores maiores; e, por fim, ${}^{3)}$ se $f$ é arbitrariamente complexa, então $\mathcal{M}_s$ é potencialmente maior (vide Fig. 4). Esse, como deve ter ficado bem claro no ponto do texto em que estamos, é um problema bastante importante no que se diz respeito ao poder de generalização dos nossos modelos $-$ e, por isso, é importante estudarmos formas de mitigá-lo. A próxima postagem fará isso, apresentando a técnica de regularização.</p>

<p>Qualquer dúvida, sugestão ou <em>feedback</em>, por favor, deixe um comentário abaixo.</p>

<blockquote>
  <p>Essa postagem faz parte de uma <a href="/categorias/#machine-learning-learning-from-data">série</a> de textos que tem o objetivo de estudar, principalmente, o curso “<a href="http://www.work.caltech.edu/telecourse.html">Learning from Data</a>”, ministrado pelo Professor Yaser Abu-Mostafa. Outros materiais utilizados serão sempre referenciados.</p>
</blockquote>

        
      </section>

      <footer class="page__meta">
        
        


        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Atualizado em:</strong> <time datetime="2020-02-06T00:00:00-03:00">February 06, 2020</time></p>
        
      </footer>

      

      
  <nav class="pagination">
    
      <a href="/implementacao-redes-neurais/" class="pagination--pager" title="[Parte 13] Implementação em Python: Redes Neurais
">Anterior</a>
    
    
      <a href="#" class="pagination--pager disabled">Próxima</a>
    
  </nav>

    </div>

    
      <div class="page__comments">
  
  
      <h4 class="page__comments-title">Deixe um comentário</h4>
      <section id="disqus_thread"></section>
    
</div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">Talvez você também goste</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/implementacao-redes-neurais/" rel="permalink">[Parte 13] Implementação em Python: Redes Neurais
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  16 minuto(s) de leitura

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Como continuação direta da parte 12 dessa série de textos, vamos, ao longo desse post, implementar uma rede neural utilizando Python. E, para testá-la, vamos resolver um problema de classificação binária de dados que não são linearmente separáveis $-$ aqui, não faremos como na parte 06, onde utilizamos transformaçõe...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/redes-neurais/" rel="permalink">[Parte 12] Redes Neurais
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  14 minuto(s) de leitura

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Ao longo desse post vamos, como temos feito para todos os modelos que discutimos até agora nessa série de textos, estudar o que são, do ponto de vista mais teórico, as redes neurais. A ideia é que, na próxima postagem, a gente consiga implementar em Python o que vamos estudar a partir desse momento.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/modelo-de-regressao-logistica/" rel="permalink">[Parte 11] Modelo de Regressão Logística &amp; Implementação em Python
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  15 minuto(s) de leitura

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Voltando a falar de modelos lineares, como os de classificação $-$ explorados nas partes 02 e 04 $-$ ou de regressão linear, a exemplo do que vimos na parte 05, iremos discutir nesse post o que é e como funciona a regressão logística. Veremos que esse novo modelo herda características das duas classes de algoritmos ...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/vies-variancia-tradeoff/" rel="permalink">[Parte 10] Trade-off entre Viés-Variância
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  7 minuto(s) de leitura

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Continuando com a discussão, introduzida na parte 09 dessa série de textos, de que deve existir um “meio termo” sobre o tamanho de $\mathcal{H}$ $-$ lembre-se: se $\mathcal{H}$ é muito grande, conseguimos diminuir o termo $E_{in}$; porém, somos penalizados na generalização do modelo para dados fora de $\mathcal{D}$....</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Informações:</strong></li>
    

    
      
        
          <li><a href="https://github.com/avramaral" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://twitter.com/avramaral" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/andre-victor-ribeiro-amaral/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin-in" aria-hidden="true"></i> LinkedIn</a></li>
        
      
        
          <li><a href="https://www.youtube.com/channel/UC5RVscKF68DWYixtJPrBccQ" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-youtube" aria-hidden="true"></i> YouTube</a></li>
        
      
    

    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 André. Desenvolvido com <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>







    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/sobreajuste/";  // Replace PAGE_URL with your page's canonical URL variable
      this.page.identifier = "/sobreajuste"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    (function() { // DON'T EDIT BELOW THIS LINE
      var d = document, s = d.createElement('script');
      s.src = 'https://blog-do-andre.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  





    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  </body>
</html>
